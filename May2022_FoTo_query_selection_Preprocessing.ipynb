{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "2qwXpCj4nzMV",
        "OlbeXz2Rnv3l",
        "8lETxYNFoAfE",
        "QzFmoO4pKOAQ",
        "xadmuhAJD_ZW",
        "bhrpzQaeELXV",
        "g_1wvTBSV8hl",
        "EAluOTUeW4gj",
        "T1AORK6UV3em",
        "kNBWvIY5itSO",
        "YPVGy3_mB5ZK",
        "9QznJmZdQmRg",
        "NY7YQxgOhgdL"
      ],
      "authorship_tag": "ABX9TyOcdRPHVHEhx1JzsrrTgbKI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanujsriv/_topic_models/blob/FoTo/May2022_FoTo_query_selection_Preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### just run these"
      ],
      "metadata": {
        "id": "2qwXpCj4nzMV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "#@title Imports, wordnet,punkt\n",
        "import nltk\n",
        "import glob\n",
        "# nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "import re\n",
        "import string\n",
        "from time import time\n",
        "import numpy as np\n",
        "import collections\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from numpy import random\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import word_tokenize,sent_tokenize\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92xPBJ7GRY-s",
        "outputId": "36b4bf35-59a9-4a6b-8da2-1e43f1212139",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     /home/grad16/sakumar/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     /home/grad16/sakumar/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title function : load / save pickle_obj\n",
        "\n",
        "### pickle\n",
        "\n",
        "import bz2\n",
        "import pickle\n",
        "!pip install pickle5\n",
        "import pickle5\n",
        "import _pickle as cPickle\n",
        "\n",
        "def save_obj(obj, name):\n",
        "    with open(name + '.pkl', 'wb') as f:\n",
        "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "def load_obj(name):\n",
        "    with open(name + '.pkl', 'rb') as f:\n",
        "        return pickle.load(f)\n",
        "\n",
        "def load_obj_pkl5(name):\n",
        "    with open(name + '.pkl', 'rb') as f:\n",
        "        return pickle5.load(f)\n",
        "\n",
        "def compressed_pickle(data,title):\n",
        "  with bz2.BZ2File(title + '.pbz2', 'w') as f:\n",
        "    cPickle.dump(data, f)\n",
        "\n",
        "def decompress_pickle(file):\n",
        " data = bz2.BZ2File(file+'.pbz2', 'rb')\n",
        " data = cPickle.load(data)\n",
        " return data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c50e17e5-9972-40e8-c237-05c758145f2a",
        "cellView": "form",
        "id": "qJ6F0q0yoC9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pickle5 in /home/student_no_backup/sakumar/miniconda3/envs/cuda/lib/python3.8/site-packages (0.0.11)\r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if 'grad16' or 'student' in os.getcwd(): local=True\n",
        "else: local = False\n",
        "if local:\n",
        "  paper='emnlp2022'\n",
        "  home_dir = '/home/grad16/sakumar/'+paper+'/dataset'\n",
        "elif not local:\n",
        "  home_dir = ''\n",
        "os.chdir(home_dir)"
      ],
      "metadata": {
        "id": "23ZPyejPsUOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###load word2vec (takes time...!)"
      ],
      "metadata": {
        "id": "OlbeXz2Rnv3l"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SMctDoHEUC4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd8c0061-bdfa-4c87-f94e-3e3f3ad94d99"
      },
      "source": [
        "from gensim import models\n",
        "if local==True:\n",
        "  if not os.path.isfile(\"/home/student_no_backup/sakumar/miniconda3/bin/GoogleNews-vectors-negative300.bin\"):\n",
        "    !wget -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"\n",
        "    !gunzip GoogleNews-vectors-negative300.bin.gz\n",
        "  word2vec_model = models.KeyedVectors.load_word2vec_format('/home/student_no_backup/sakumar/miniconda3/bin/GoogleNews-vectors-negative300.bin', binary=True)\n",
        "\n",
        "if local==False:\n",
        "  # https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?resourcekey=0-wjGZdNAUop6WykTtMip30g\n",
        "  !gdown --id 0B7XkCwpI5KDYNlNUTTlSS21pQmM\n",
        "  ## OR (not working)\n",
        "  # !wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=0B7XkCwpI5KDYNlNUTTlSS21pQmM' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=0B7XkCwpI5KDYNlNUTTlSS21pQmM\" -O GoogleNews-vectors-negative300.bin.gz && rm -rf /tmp/cookies.txt\n",
        "  ## OR (not working)\n",
        "  # !wget -N -c \"https://raw.githubusercontent.com/mmihaltz/word2vec-GoogleNews-vectors/master/GoogleNews-vectors-negative300.bin.gz\"\n",
        "  !gunzip GoogleNews-vectors-negative300.bin.gz\n",
        "  word2vec_model = models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-88-2f9fb8e5512c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'wget -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'gunzip GoogleNews-vectors-negative300.bin.gz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0mword2vec_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/student_no_backup/sakumar/miniconda3/bin/GoogleNews-vectors-negative300.bin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlocal\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniconda3/envs/cuda/lib/python3.8/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m   1434\u001b[0m         \"\"\"\n\u001b[1;32m   1435\u001b[0m         \u001b[0;31m# from gensim.models.word2vec import load_word2vec_format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1436\u001b[0;31m         return _load_word2vec_format(\n\u001b[0m\u001b[1;32m   1437\u001b[0m             \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1438\u001b[0m             limit=limit, datatype=datatype)\n",
            "\u001b[0;32m~/miniconda3/envs/cuda/lib/python3.8/site-packages/gensim/models/utils_any2vec.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m    203\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m                     \u001b[0mch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0mch\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34mb' '\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m                         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mch\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34mb''\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing"
      ],
      "metadata": {
        "id": "8lETxYNFoAfE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing helper functions"
      ],
      "metadata": {
        "id": "-P1J3DlfqCcF"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zf1p8IblDwIP"
      },
      "source": [
        "stem = PorterStemmer()\n",
        "wnl = WordNetLemmatizer()\n",
        "stopwords = ['http','la','wa','will','fa','ha','pa','co','v','said','cant','better','well','going','will','would','know','dont','get','like','think','im',\"also\",\"said\",\"a\", \"able\", \"about\", \"above\", \"abst\", \"accordance\", \"according\", \"accordingly\", \"across\", \"act\", \"actually\", \"added\", \"adj\", \"affected\", \"affecting\", \"affects\", \"after\", \"afterwards\", \"again\", \"against\", \"ah\", \"all\", \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\", \"am\", \"among\", \"amongst\", \"an\", \"and\", \"announce\", \"another\", \"any\", \"anybody\", \"anyhow\", \"anymore\", \"anyone\", \"anything\", \"anyway\", \"anyways\", \"anywhere\", \"apparently\", \"approximately\", \"are\", \"aren\", \"arent\", \"arise\", \"around\", \"as\", \"aside\", \"ask\", \"asking\", \"at\", \"auth\", \"available\", \"away\", \"awfully\", \"b\", \"back\", \"be\", \"became\", \"because\", \"become\", \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"begin\", \"beginning\", \"beginnings\", \"begins\", \"behind\", \"being\", \"believe\", \"below\", \"beside\", \"besides\", \"between\", \"beyond\", \"biol\", \"both\", \"brief\", \"briefly\", \"but\", \"by\", \"c\", \"ca\", \"came\", \"can\", \"cannot\", \"can't\", \"cause\", \"causes\", \"certain\", \"certainly\", \"co\", \"com\", \"come\", \"comes\", \"contain\", \"containing\", \"contains\", \"could\", \"couldnt\", \"d\", \"date\", \"did\", \"didn't\", \"different\", \"do\", \"does\", \"doesn't\", \"doing\", \"done\", \"don't\", \"down\", \"downwards\", \"due\", \"during\", \"e\", \"each\", \"ed\", \"edu\", \"effect\", \"eg\", \"eight\", \"eighty\", \"either\", \"else\", \"elsewhere\", \"end\", \"ending\", \"enough\", \"especially\", \"et\", \"et-al\", \"etc\", \"even\", \"ever\", \"every\", \"everybody\", \"everyone\", \"everything\", \"everywhere\", \"ex\", \"except\", \"f\", \"far\", \"few\", \"ff\", \"fifth\", \"first\", \"five\", \"fix\", \"followed\", \"following\", \"follows\", \"for\", \"former\", \"formerly\", \"forth\", \"found\", \"four\", \"from\", \"further\", \"furthermore\", \"g\", \"gave\", \"get\", \"gets\", \"getting\", \"give\", \"given\", \"gives\", \"giving\", \"go\", \"goes\", \"gone\", \"got\", \"gotten\", \"h\", \"had\", \"happens\", \"hardly\", \"has\", \"hasn't\", \"have\", \"haven't\", \"having\", \"he\", \"hed\", \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"heres\", \"hereupon\", \"hers\", \"herself\", \"hes\", \"hi\", \"hid\", \"him\", \"himself\", \"his\", \"hither\", \"home\", \"how\", \"howbeit\", \"however\", \"hundred\", \"i\", \"id\", \"ie\", \"if\", \"i'll\", \"im\", \"immediate\", \"immediately\", \"importance\", \"important\", \"in\", \"inc\", \"indeed\", \"index\", \"information\", \"instead\", \"into\", \"invention\", \"inward\", \"is\", \"isn't\", \"it\", \"itd\", \"it'll\", \"its\", \"itself\", \"i've\", \"j\", \"just\", \"k\", \"keep  keeps\", \"kept\", \"kg\", \"km\", \"know\", \"known\", \"knows\", \"l\", \"largely\", \"last\", \"lately\", \"later\", \"latter\", \"latterly\", \"least\", \"less\", \"lest\", \"let\", \"lets\", \"like\", \"liked\", \"likely\", \"line\", \"little\", \"'ll\", \"look\", \"looking\", \"looks\", \"ltd\", \"m\", \"made\", \"mainly\", \"make\", \"makes\", \"many\", \"may\", \"maybe\", \"me\", \"mean\", \"means\", \"meantime\", \"meanwhile\", \"merely\", \"mg\", \"might\", \"million\", \"miss\", \"ml\", \"more\", \"moreover\", \"most\", \"mostly\", \"mr\", \"mrs\", \"much\", \"mug\", \"must\", \"my\", \"myself\", \"n\", \"na\", \"name\", \"namely\", \"nay\", \"nd\", \"near\", \"nearly\", \"necessarily\", \"necessary\", \"need\", \"needs\", \"neither\", \"never\", \"nevertheless\", \"new\", \"next\", \"nine\", \"ninety\", \"no\", \"nobody\", \"non\", \"none\", \"nonetheless\", \"noone\", \"nor\", \"normally\", \"nos\", \"not\", \"noted\", \"nothing\", \"now\", \"nowhere\", \"o\", \"obtain\", \"obtained\", \"obviously\", \"of\", \"off\", \"often\", \"oh\", \"ok\", \"okay\", \"old\", \"omitted\", \"on\", \"once\", \"one\", \"ones\", \"only\", \"onto\", \"or\", \"ord\", \"other\", \"others\", \"otherwise\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"outside\", \"over\", \"overall\", \"owing\", \"own\", \"p\", \"page\", \"pages\", \"part\", \"particular\", \"particularly\", \"past\", \"per\", \"perhaps\", \"placed\", \"please\", \"plus\", \"poorly\", \"possible\", \"possibly\", \"potentially\", \"pp\", \"predominantly\", \"present\", \"previously\", \"primarily\", \"probably\", \"promptly\", \"proud\", \"provides\", \"put\", \"q\", \"que\", \"quickly\", \"quite\", \"qv\", \"r\", \"ran\", \"rather\", \"rd\", \"re\", \"readily\", \"really\", \"recent\", \"recently\", \"ref\", \"refs\", \"regarding\", \"regardless\", \"regards\", \"related\", \"relatively\", \"research\", \"respectively\", \"resulted\", \"resulting\", \"results\", \"right\", \"run\", \"s\", \"said\", \"same\", \"saw\", \"say\", \"saying\", \"says\", \"sec\", \"section\", \"see\", \"seeing\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"seen\", \"self\", \"selves\", \"sent\", \"seven\", \"several\", \"shall\", \"she\", \"shed\", \"she'll\", \"shes\", \"should\", \"shouldn't\", \"show\", \"showed\", \"shown\", \"showns\", \"shows\", \"significant\", \"significantly\", \"similar\", \"similarly\", \"since\", \"six\", \"slightly\", \"so\", \"some\", \"somebody\", \"somehow\", \"someone\", \"somethan\", \"something\", \"sometime\", \"sometimes\", \"somewhat\", \"somewhere\", \"soon\", \"sorry\", \"specifically\", \"specified\", \"specify\", \"specifying\", \"still\", \"stop\", \"strongly\", \"sub\", \"substantially\", \"successfully\", \"such\", \"sufficiently\", \"suggest\", \"sup\", \"sure    t\", \"take\", \"taken\", \"taking\", \"tell\", \"tends\", \"th\", \"than\", \"thank\", \"thanks\", \"thanx\", \"that\", \"that'll\", \"thats\", \"that've\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\", \"thered\", \"therefore\", \"therein\", \"there'll\", \"thereof\", \"therere\", \"theres\", \"thereto\", \"thereupon\", \"there've\", \"these\", \"they\", \"theyd\", \"they'll\", \"theyre\", \"they've\", \"think\", \"this\", \"those\", \"thou\", \"though\", \"thoughh\", \"thousand\", \"throug\", \"through\", \"throughout\", \"thru\", \"thus\", \"til\", \"tip\", \"to\", \"together\", \"too\", \"took\", \"toward\", \"towards\", \"tried\", \"tries\", \"truly\", \"try\", \"trying\", \"ts\", \"twice\", \"two\", \"u\", \"un\", \"under\", \"unfortunately\", \"unless\", \"unlike\", \"unlikely\", \"until\", \"unto\", \"up\", \"upon\", \"ups\", \"us\", \"use\", \"used\", \"useful\", \"usefully\", \"usefulness\", \"uses\", \"using\", \"usually\", \"v\", \"value\", \"various\", \"'ve\", \"very\", \"via\", \"viz\", \"vol\", \"vols\", \"vs\", \"w\", \"want\", \"wants\", \"was\", \"wasnt\", \"way\", \"we\", \"wed\", \"welcome\", \"we'll\", \"went\", \"were\", \"werent\", \"we've\", \"what\", \"whatever\", \"what'll\", \"whats\", \"when\", \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"wheres\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whim\", \"whither\", \"who\", \"whod\", \"whoever\", \"whole\", \"who'll\", \"whom\", \"whomever\", \"whos\", \"whose\", \"why\", \"widely\", \"willing\", \"wish\", \"with\", \"within\", \"without\", \"wont\", \"words\", \"world\", \"would\", \"wouldnt\", \"www\", \"x\", \"y\", \"yes\", \"yet\", \"you\", \"youd\", \"you'll\", \"your\", \"youre\", \"yours\", \"yourself\", \"yourselves\", \"you've\", \"z\", \"zero\"]\n",
        "\n",
        "def preprocessing(doc,word2vec_model,my_punctuation,min_doc_length,min_word_length=0):\n",
        "    doc = doc.lower()\n",
        "    doc = doc.translate(str.maketrans(string.punctuation, ' '*len(string.punctuation)))  # string.punctuation[6] = ' won't --> won , don't --> don\n",
        "    doc = word_tokenize(doc)\n",
        "    doc = filter(lambda x: x not in my_punctuation, doc)\n",
        "    doc = filter(lambda x:not x.isdigit(), doc)\n",
        "    doc = [wnl.lemmatize(w.lower()) for w in doc]\n",
        "    #doc = [stem.stem(w) for w in doc]\n",
        "    doc = filter(lambda x:x not in stopwords, doc)\n",
        "    doc = filter(lambda x: x in word2vec_model.vocab or x in \".\",doc)\n",
        "    doc = list(doc)\n",
        "    doc = [e for e in doc if len(e)>=min_word_length]\n",
        "    if len(doc)>=min_doc_length:\n",
        "      doc = ' '.join(e for e in doc)\n",
        "    else:\n",
        "      doc = ''\n",
        "    return doc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vocab_filtered_data(doc,vocab):\n",
        "  doc = word_tokenize(doc)\n",
        "  doc = filter(lambda x: x in vocab, doc)\n",
        "  doc = ' '.join(e for e in doc)\n",
        "  return doc\n",
        "\n",
        "def flatten(t):\n",
        "    return [item for sublist in t for item in sublist]\n",
        "\n",
        "\n",
        "def docs_labels_preprocessing(data_preprocessed,data_preprocessed_labels,word2vec_model,min_doc_len,min_word_length,max_features):\n",
        "  embeddings = {}\n",
        "\n",
        "  vectorizer = CountVectorizer(max_features=max_features,dtype=np.uint8)\n",
        "  train_vec = vectorizer.fit_transform(data_preprocessed).toarray()\n",
        "  vocab = vectorizer.vocabulary_\n",
        "\n",
        "  nonzeros_indexes = np.where(train_vec.any(1))[0]\n",
        "  data_preprocessed = [data_preprocessed[i] for i in nonzeros_indexes]\n",
        "  data_preprocessed_labels = [data_preprocessed_labels[i] for i in nonzeros_indexes]\n",
        "\n",
        "  for i in range(len(data_preprocessed)):\n",
        "    data_preprocessed[i] = vocab_filtered_data(data_preprocessed[i],vocab)\n",
        "\n",
        "  data_preprocessed_f = [data_preprocessed[i] for i in range(len(data_preprocessed)) if len(data_preprocessed[i].split(' '))>=min_doc_len]\n",
        "  data_preprocessed_labels_f = [data_preprocessed_labels[i] for i in range(len(data_preprocessed)) if len(data_preprocessed[i].split(' '))>=min_doc_len]\n",
        "\n",
        "  vectorizer_f = CountVectorizer(dtype=np.uint8)\n",
        "  vectorizer_f.fit_transform(data_preprocessed_f)\n",
        "  vocab_f = vectorizer.vocabulary_\n",
        "\n",
        "  for f in vocab_f:\n",
        "    embeddings[f] = word2vec_model[f]\n",
        "  return data_preprocessed_f,data_preprocessed_labels_f,embeddings,vocab_f\n",
        "\n",
        "\n",
        "def docs_labels_preprocessing_sent(docs,labels,word2vec_model,min_doc_len,min_word_length,max_features,split_to_sentences):\n",
        "  data_preprocessed = []\n",
        "  data_preprocessed_labels = []\n",
        "  embeddings = {}\n",
        "  data_preprocessed_f = []\n",
        "  data_preprocessed_labels_f = []\n",
        "  vocab_f = {}\n",
        "\n",
        "  for i in range(len(docs)):\n",
        "\n",
        "    doc = preprocessing(docs[i],word2vec_model,string.punctuation,min_doc_len,min_word_len)\n",
        "    if len(doc)!=0:\n",
        "      data_preprocessed.append(doc)\n",
        "      data_preprocessed_labels.append(labels[i])\n",
        "\n",
        "  if len(data_preprocessed)!=0:\n",
        "    vectorizer = CountVectorizer(max_features=max_features,dtype=np.float32)\n",
        "    train_vec = vectorizer.fit_transform(data_preprocessed).toarray()\n",
        "    vocab = vectorizer.vocabulary_\n",
        "\n",
        "    nonzeros_indexes = np.where(train_vec.any(1))[0]\n",
        "    data_preprocessed = [data_preprocessed[i] for i in nonzeros_indexes]\n",
        "    data_preprocessed_labels = [data_preprocessed_labels[i] for i in nonzeros_indexes]\n",
        "\n",
        "    for i in range(len(data_preprocessed)):\n",
        "      data_preprocessed[i] = vocab_filtered_data(data_preprocessed[i],vocab)\n",
        "\n",
        "\n",
        "    if not split_to_sentences:\n",
        "      data_preprocessed_f = [data_preprocessed[i] for i in range(len(data_preprocessed)) if len(data_preprocessed[i].split(' '))>=min_doc_len]\n",
        "      data_preprocessed_labels_f = [data_preprocessed_labels[i] for i in range(len(data_preprocessed)) if len(data_preprocessed[i].split(' '))>=min_doc_len]\n",
        "\n",
        "\n",
        "    if split_to_sentences:\n",
        "      data_preprocessed_f = [data_preprocessed[i] for i in range(len(data_preprocessed)) if len(data_preprocessed)>1 or len(data_preprocessed[i].split(' '))>=min_doc_len]\n",
        "      data_preprocessed_labels_f = [data_preprocessed_labels[i] for i in range(len(data_preprocessed)) if len(data_preprocessed)>1 or len(data_preprocessed[i].split(' '))>=min_doc_len]\n",
        "\n",
        "\n",
        "    if len(data_preprocessed_f)!=0:\n",
        "      vectorizer_f = CountVectorizer(dtype=np.uint8)\n",
        "      vectorizer_f.fit_transform(data_preprocessed_f)\n",
        "      vocab_f = vectorizer.vocabulary_\n",
        "\n",
        "    for f in vocab:\n",
        "      embeddings[f] = word2vec_model[f]\n",
        "  return data_preprocessed_f,data_preprocessed_labels_f,embeddings,vocab_f\n",
        "\n",
        "def get_corpus_len_info(data_preprocessed,name):\n",
        "  len_docs = [len(d.split(\" \")) for d in data_preprocessed]\n",
        "  print('name :',name)\n",
        "  print('min, mean, max docs len  : ',str(np.min(len_docs))+', '+str(np.mean(len_docs).round(2))+', '+str(np.max(len_docs))+'\\n\\n')"
      ],
      "metadata": {
        "id": "_4SoOff3t5gr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### bbc"
      ],
      "metadata": {
        "id": "QzFmoO4pKOAQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_bbc():\n",
        "  dtype='short'\n",
        "  os.system('wget -N https://www.dropbox.com/s/vunli21d312x55g/bbc.zip')\n",
        "  os.system('wget -N https://www.dropbox.com/s/h6y9zfdb76gl4uz/bbc-fulltext.zip')\n",
        "  os.system('unzip bbc.zip')\n",
        "  os.system('unzip bbc-fulltext.zip')\n",
        "\n",
        "  # BBC Docs -\n",
        "  corpus = []\n",
        "  subfolders = [f.path for f in os.scandir(os.getcwd()+'/bbc') if f.is_dir()]\n",
        "  subfolders = sorted(subfolders)\n",
        "  for s in subfolders:\n",
        "    files_list = sorted(glob.glob(s+\"/*.txt\"))\n",
        "    for file in files_list:\n",
        "      with open(file, \"rb\") as f:\n",
        "        content = f.readlines()\n",
        "        if dtype == 'short':\n",
        "          content = [content[0],content[2]] # headlines and abstracts\n",
        "        content = [x.strip().lower().decode('ISO-8859-1') for x in content]\n",
        "        corpus.append(' '.join(content).strip())\n",
        "\n",
        "  # BBC_labels -\n",
        "  with open(\"bbc.classes\", \"r\") as f:\n",
        "    content = f.readlines()\n",
        "    content = [x.strip()[::-1][0] for x in content]\n",
        "    labels = content[4:]\n",
        "    label_dict = {'0':'business','1':'entertainment','2':'politics','3':'sport','4':'tech'}\n",
        "  for l in range(len(labels)):\n",
        "    labels[l] = label_dict[labels[l]]\n",
        "  return corpus,labels"
      ],
      "metadata": {
        "id": "_8lvoYtSEWsO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### searchsnippet\n"
      ],
      "metadata": {
        "id": "xadmuhAJD_ZW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_searchsnippet():\n",
        "  os.system('wget http://jwebpro.sourceforge.net/data-web-snippets.tar.gz')\n",
        "  os.system('tar -xvzf data-web-snippets.tar.gz')\n",
        "  corpus = []\n",
        "  labels=[]\n",
        "  with open(\"./data-web-snippets/train.txt\", \"r\") as f:\n",
        "    content = f.readlines()\n",
        "    content = [x.split('\\n')[0] for x in content]\n",
        "    labels_train = [x.split(' ')[-1] for x in content]\n",
        "    content = [' '.join(x.split(' ')[:-1]) for x in content]\n",
        "    corpus.extend(content)\n",
        "    labels.extend(labels_train)\n",
        "\n",
        "  with open(\"./data-web-snippets/test.txt\", \"r\") as f:\n",
        "    content = f.readlines()\n",
        "    content = [x.split('\\n')[0] for x in content]\n",
        "    labels_test = [x.split(' ')[-1] for x in content]\n",
        "    content = [' '.join(x.split(' ')[:-1]) for x in content]\n",
        "    corpus.extend(content)\n",
        "    labels.extend(labels_test)\n",
        "  return corpus,labels"
      ],
      "metadata": {
        "id": "8YCM5480ECDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### yahooanswers"
      ],
      "metadata": {
        "id": "bhrpzQaeELXV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "def get_yahooanswers():\n",
        "  dtype = 'short'\n",
        "  gdrive_fileid = \"\"\"0Bz8a_Dbh9Qhbd2JNdDBsQUdocVU\"\"\"\n",
        "  gdrive_filename = \"\"\"yahoo_answers_csv.tar.gz\"\"\"\n",
        "\n",
        "  dir= home_dir+'/yahoo_answers_csv'\n",
        "  if not os.path.exists(dir) and not os.path.isfile(gdrive_filename):\n",
        "    !wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=0Bz8a_Dbh9Qhbd2JNdDBsQUdocVU' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=0Bz8a_Dbh9Qhbd2JNdDBsQUdocVU\" -O yahoo_answers_csv.tar.gz && rm -rf /tmp/cookies.txt\n",
        "    os.system(\"tar -xvzf yahoo_answers_csv.tar.gz\")\n",
        "  # os.makedirs(home_dir+dir,exist_ok=True)\n",
        "  os.chdir(dir)\n",
        "  dict_labels = {'1':'Society & Culture', '2': 'Science & Mathematics',\n",
        "                '3': 'Health', '4': 'Education & Reference', '5' : 'Computers & Internet',\n",
        "                '6': 'Sports', '7': 'Business & Finance', '8': 'Entertainment & Music',\n",
        "                '9': 'Family & Relationships' , '10':'Politics & Government'}\n",
        "\n",
        "  labels = []\n",
        "  corpus = []\n",
        "  files = ['train.csv','test.csv']\n",
        "  # files = ['train.csv']\n",
        "  count = 0\n",
        "  for f in files:\n",
        "    with open(f, newline='') as csvfile:\n",
        "        reader = csv.reader(csvfile)\n",
        "  #       # next(reader)\n",
        "        if dtype == 'short':\n",
        "          for row in reader:\n",
        "            count = count+1\n",
        "            labels.append(dict_labels[row[0]])\n",
        "            # corpus.append(' '.join(t for t in row[1:]))\n",
        "            corpus.append(' '.join(t for t in row[1:3])) # SHORT\n",
        "        elif dtype == \"full\":\n",
        "          for row in reader:\n",
        "            count = count+1\n",
        "            labels.append(dict_labels[row[0]])\n",
        "            corpus.append(' '.join(t for t in row[1:]))\n",
        "  return corpus,labels"
      ],
      "metadata": {
        "id": "mLIu2noVENRO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### nfcorpus"
      ],
      "metadata": {
        "id": "g_1wvTBSV8hl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.cl.uni-heidelberg.de/statnlpgroup/nfcorpus/"
      ],
      "metadata": {
        "id": "6JSBPhWEQ-n_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## nfcorpus\n",
        "import pandas as pd\n",
        "import glob\n",
        "\n",
        "def get_nfcorpus():\n",
        "  !wget -N -c https://www.cl.uni-heidelberg.de/statnlpgroup/nfcorpus/nfcorpus.tar.gz\n",
        "  !tar -xvzf nfcorpus.tar.gz\n",
        "\n",
        "  os.chdir(home_dir+\"/nfcorpus/raw\")\n",
        "\n",
        "  with open('doc_dump.txt', \"r\") as f: # titles only\n",
        "    content = f.readlines()\n",
        "    doc_dump_ids = [c.split('\\t')[0] for c in content]\n",
        "    doc_dump_titles = [c.split('\\t')[2].split('- PubMed')[0].strip() for c in content]\n",
        "  doc_dump_dict = dict(zip(doc_dump_ids, doc_dump_titles))\n",
        "  get_corpus_len_info(doc_dump_titles,'doc_dump.txt')\n",
        "\n",
        "  os.chdir(home_dir+\"/nfcorpus\")\n",
        "  queries_file_list = ['dev.titles.queries','train.titles.queries','test.titles.queries']\n",
        "\n",
        "  query_docs_ids, queries_docs = [],[]\n",
        "  for file in queries_file_list:\n",
        "      with open(file, \"r\") as f:\n",
        "        content = f.readlines()\n",
        "        query_docs_ids.extend([c.split('\\t')[0] for c in content])\n",
        "        queries_docs.extend([c.split('\\t')[1].strip() for c in content])\n",
        "  query_dict = dict(zip(query_docs_ids, queries_docs))\n",
        "\n",
        "  qrels_file_list = ['dev.2-1-0.qrel','train.2-1-0.qrel','test.2-1-0.qrel']\n",
        "\n",
        "  path = home_dir+\"/nfcorpus\" # use your path\n",
        "  all_files = glob.glob(path + \"/*.2-1-0.qrel\")\n",
        "\n",
        "  li = []\n",
        "\n",
        "  for filename in all_files:\n",
        "      df = pd.read_csv(filename,sep ='\\t',header=None)\n",
        "      li.append(df)\n",
        "\n",
        "  frame = pd.concat(li, axis=0, ignore_index=True)\n",
        "  frame.columns = ['QUERY_ID', '0', 'DOC_ID', 'RELEVANCE_LEVEL']\n",
        "  frame = frame.drop(['0'],axis=1)\n",
        "  frame['QUERY'] = frame['QUERY_ID'].apply(lambda x:  query_dict[x])\n",
        "  frame['DOC_TITLE'] = frame['DOC_ID'].apply(lambda x:  doc_dump_dict[x])\n",
        "\n",
        "  frame.replace('', np.nan, inplace=True)\n",
        "  frame.isnull().sum()\n",
        "  frame = frame.dropna().reset_index(drop=True)\n",
        "  return frame"
      ],
      "metadata": {
        "id": "rp1-vtjqW_Ft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# docs_idx = frame[['DOC_TITLE_PREPROCESSED']].drop_duplicates().reset_index(drop=True).index.tolist()\n",
        "# docs_label_df = frame.iloc[docs_idx]\n",
        "# docs = docs_label_df['DOC_TITLE_PREPROCESSED'].values\n",
        "# labels = docs_label_df['DOC_ID'].values#.apply(lambda x: x.split('-')[0]).values\n",
        "\n",
        "# min_doc_len,min_word_len = 3,3\n",
        "# # preprocessing('Hello !! bro> aa',word2vec_model,string.punctuation,min_doc_len,min_word_len)\n",
        "# frame['DOC_TITLE_PREPROCESSED'] = frame['DOC_TITLE'].apply(lambda x: preprocessing(x,word2vec_model,string.punctuation,min_doc_len,min_word_len))\n",
        "\n",
        "# docs_label_df[docs_label_df['RELEVANCE_LEVEL']==2][['QUERY','DOC_ID','RELEVANCE_LEVEL']].values\n",
        "# docs_label_df[docs_label_df['QUERY']=='organic milk and prostate cancer'][['RELEVANCE_LEVEL','DOC_TITLE_PREPROCESSED']].values\n",
        "# foods for macular degeneration , how to boost the benefits of exercise, organic milk and prostate cancer"
      ],
      "metadata": {
        "id": "uXC2e-6V2GSq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### twitter"
      ],
      "metadata": {
        "id": "EAluOTUeW4gj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_opinions_twitter():\n",
        "  ## twitter\n",
        "  !wget -N -c https://www.dropbox.com/s/obu9dve4ul5c0up/Corpus.tar\n",
        "  !tar -xvzf Corpus.tar\n",
        "  os.chdir(home_dir+\"/Corpus\")\n",
        "\n",
        "  with open('DATA', \"r\") as f: # titles only\n",
        "    content = f.readlines()\n",
        "    queries = [x.split('#')[1].strip() for x in content if \"#\" in x]\n",
        "    tweet_ids , relevance = map(list,zip(*[x.split('\\t') for x in content if \"#\" not in x]))\n",
        "    relevance = [x.strip() for x in relevance]\n",
        "\n",
        "  os.chdir(home_dir+'/etc')\n",
        "  tweet_text = decompress_pickle('tweet_text_5000')\n",
        "  os.chdir(home_dir)\n",
        "  repeated_queries = np.repeat(queries,100)\n",
        "  tweets= []\n",
        "  f_queries = []\n",
        "  f_relevance = []\n",
        "  f_tweet_ids = []\n",
        "  for i in range(len(tweet_text)):\n",
        "    if len(tweet_text[i])==0:\n",
        "      pass\n",
        "    else:\n",
        "      tweets.append(tweet_text[i])\n",
        "      f_queries.append(repeated_queries[i])\n",
        "      f_relevance.append(relevance[i])\n",
        "      f_tweet_ids.append(tweet_ids[i])\n",
        "  labels = len(tweets)*['tweets']\n",
        "  df = pd.DataFrame()\n",
        "  df['tweet_ids'] = f_tweet_ids\n",
        "  df['tweets'] = tweets\n",
        "  df['labels'] = labels\n",
        "  df['queries'] = f_queries\n",
        "  df['relevance'] = f_relevance\n",
        "  df.replace('', np.nan, inplace=True)\n",
        "  df.isnull().sum()\n",
        "  df = df.dropna().reset_index(drop=True)\n",
        "  get_corpus_len_info(tweets,'tweets_opinion_retreival')\n",
        "  return df"
      ],
      "metadata": {
        "id": "p23QYDo749fS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "API Key\n",
        "HQBwNF26a7scxTkRoAsCdTr4E\n",
        "\n",
        "\n",
        "API Key Secret\n",
        "eEQ5nkwl1C7qTCg5wO7TesPeWVAOpJBGMPtXQZwE8SArbbNLTq\n",
        "\n",
        "Bearer Token\n",
        "AAAAAAAAAAAAAAAAAAAAANyvcQEAAAAAnhZigGSgKGTrht0vpSAyit3mbyE%3DCOXrscMFyXzNDCDYg9m0SkpSsC3KUnn1KUGcTUC5c5YL7bTnEQ\n",
        "\n",
        "Access Token\n",
        "1103974251491663873-NOZFV4f0MoAWkhtaG8vVtoORfApe34\n",
        "\n",
        "Access Token Secret\n",
        "ATFYBvajprMvhyKRntNpLQEe6qKjjmIODaMDySdZqFpHB\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "oUiH_aQ0jduI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install tweepy\n",
        "############## 1 #################\n",
        "# def lookup_tweets(tweet_IDs, api):\n",
        "#     full_tweets = []\n",
        "#     tweet_count = len(tweet_IDs)\n",
        "#     try:\n",
        "#         for i in range((tweet_count / 100) + 1):\n",
        "#             # Catch the last group if it is less than 100 tweets\n",
        "#             end_loc = min((i + 1) * 100, tweet_count)\n",
        "#             full_tweets.extend(\n",
        "#                 api.statuses_lookup(id=tweet_IDs[i * 100:end_loc])\n",
        "#             )\n",
        "#         return full_tweets\n",
        "#     except:\n",
        "#         print('Something went wrong, quitting...')\n",
        "\n",
        "\n",
        "# consumer_key = \"HQBwNF26a7scxTkRoAsCdTr4E\"\n",
        "# consumer_secret = \"eEQ5nkwl1C7qTCg5wO7TesPeWVAOpJBGMPtXQZwE8SArbbNLTq\"\n",
        "# access_token = \"1103974251491663873-NOZFV4f0MoAWkhtaG8vVtoORfApe34\"\n",
        "# access_token_secret = \"ATFYBvajprMvhyKRntNpLQEe6qKjjmIODaMDySdZqFpHB\"\n",
        "\n",
        "# auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
        "# auth.set_access_token(access_token, access_token_secret)\n",
        "\n",
        "# api = tweepy.API(auth, wait_on_rate_limit=True)\n",
        "# results = lookup_tweets(tweet_ids, api)\n",
        "\n",
        "# # for tweet in results:\n",
        "# #     if tweet:\n",
        "# #         print(tweet.text)\n",
        "\n",
        "########## 2 ###########\n",
        "# import time\n",
        "# import tweepy\n",
        "# tweet_text=[]\n",
        "# consumer_key = \"HQBwNF26a7scxTkRoAsCdTr4E\"\n",
        "# consumer_secret = \"eEQ5nkwl1C7qTCg5wO7TesPeWVAOpJBGMPtXQZwE8SArbbNLTq\"\n",
        "# access_token = \"1103974251491663873-NOZFV4f0MoAWkhtaG8vVtoORfApe34\"\n",
        "# access_token_secret = \"ATFYBvajprMvhyKRntNpLQEe6qKjjmIODaMDySdZqFpHB\"\n",
        "\n",
        "# auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
        "# auth.set_access_token(access_token, access_token_secret)\n",
        "\n",
        "# # calling the api\n",
        "# api = tweepy.API(auth, wait_on_rate_limit=True)\n",
        "# for tweet_id in tweet_ids:\n",
        "#   try:\n",
        "#     time.sleep(1)\n",
        "#     # fetching the status\n",
        "#     status = api.get_status(tweet_id)\n",
        "\n",
        "#     # fetching the text attribute\n",
        "#     text = status.text\n",
        "#   except:\n",
        "#     text=''\n",
        "#   tweet_text.append(text)\n",
        "\n",
        "############# 3 ################\n",
        "# import tweepy\n",
        "# CONSUMER_KEY = \"HQBwNF26a7scxTkRoAsCdTr4E\"\n",
        "# CONSUMER_SECRET = \"eEQ5nkwl1C7qTCg5wO7TesPeWVAOpJBGMPtXQZwE8SArbbNLTq\"\n",
        "# OAUTH_TOKEN = \"1103974251491663873-NOZFV4f0MoAWkhtaG8vVtoORfApe34\"\n",
        "# OAUTH_TOKEN_SECRET = \"ATFYBvajprMvhyKRntNpLQEe6qKjjmIODaMDySdZqFpHB\"\n",
        "\n",
        "# auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\n",
        "# auth.set_access_token(OAUTH_TOKEN, OAUTH_TOKEN_SECRET)\n",
        "# api = tweepy.API(auth)\n",
        "# tweet_text=[]\n",
        "# for tweet_id in tweet_ids:\n",
        "#   try:\n",
        "#     tweet = api.get_status(tweet_id)\n",
        "#     tweet_text.append(tweet.text)\n",
        "#   except:\n",
        "#     tweet_text.append(\"\")\n",
        "# # compressed_pickle(tweet_text,'tweet_text')\n",
        "# compressed_pickle(tweet_text,'tweet_text_'+str(len(tweet_text)))\n"
      ],
      "metadata": {
        "id": "JLfREjnnw0bx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# failed_tweet_ids = []\n",
        "# for t in range(len(tweet_text)):\n",
        "#   if (len(tweet_text[t])==0):\n",
        "#     failed_tweet_ids.append(tweet_ids[t])\n",
        "# compressed_pickle(failed_tweet_ids,'failed_tweet_ids')"
      ],
      "metadata": {
        "id": "BUzVx2UbjBAn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from collections import Counter\n",
        "# Counter(df['queries'].values)\n",
        "# df[df['queries']=='iran nuclear'][['relevance','tweets_preprocessed']].values\n",
        "\n",
        "# # Counter(df['queries'].values)\n",
        "# Counter(df[df['queries']=='big bang']['relevance'].values)"
      ],
      "metadata": {
        "id": "kyMxBTBCnhOl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Generation begins here.."
      ],
      "metadata": {
        "id": "T1AORK6UV3em"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_to_get = 'opinions_twitter' # bbc,searchsnippet,yahooanswers,nfcorpus, opinions_twitter\n",
        "dtype = 'short'"
      ],
      "metadata": {
        "id": "oLbk8lNyKRBg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(home_dir)\n",
        "%pwd"
      ],
      "metadata": {
        "id": "4DHq194I2b0O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b785e9e1-a0bd-4e0d-c2fd-eb772228241d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/home/grad16/sakumar/emnlp2022/dataset'"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if data_to_get == 'bbc':\n",
        "  docs,labels = get_bbc()\n",
        "elif data_to_get == 'searchsnippet':\n",
        "  docs,labels = get_searchsnippet()\n",
        "elif data_to_get == 'yahooanswers':\n",
        "  docs,labels = get_yahooanswers()\n",
        "elif data_to_get == 'nfcorpus':\n",
        "  frame = get_nfcorpus()\n",
        "  docs = frame['DOC_TITLE'].values\n",
        "  doc_id = frame['DOC_ID'].apply(lambda x: x.split('-')[1]).values\n",
        "  labels = frame['DOC_ID'].apply(lambda x: x.split('-')[0]).values\n",
        "  queries = frame['QUERY'].values\n",
        "  relevance = frame['RELEVANCE_LEVEL'].values\n",
        "\n",
        "elif data_to_get == 'opinions_twitter':\n",
        "  frame = get_opinions_twitter()\n",
        "  docs = frame['tweets'].values\n",
        "  labels = frame['labels'].values\n",
        "  queries = frame['queries'].values\n",
        "  relevance = frame['relevance'].values"
      ],
      "metadata": {
        "id": "mMUVviIGG10W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98bdc115-61a4-40fa-aba4-7e140990f843"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-06-17 15:30:40--  https://www.dropbox.com/s/obu9dve4ul5c0up/Corpus.tar\r\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.8.18, 2620:100:601b:18::a27d:812\r\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.8.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/obu9dve4ul5c0up/Corpus.tar [following]\n",
            "--2022-06-17 15:30:40--  https://www.dropbox.com/s/raw/obu9dve4ul5c0up/Corpus.tar\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc616656e52d68dcc7d1eaaa9f8b.dl.dropboxusercontent.com/cd/0/inline/BnYTpFMd6Uri7bT5F7B-znTiC4Idx1fHbVGk85TMUZ1mLmg8_X7N-7nRGLc1fv2kXc0Hrd3Hx2DwlWIVjIc4UaHNC7ihJ4Eoy-mRcul90k87s6UvWLPVC_P4ElgUu4xU4St6Z4FDOhOyc_EQO0M6UkKs3Bg-slSMOTyN3P55FqkUPG2kOLWUNCGck0_Umf-saUY/file# [following]\n",
            "--2022-06-17 15:30:40--  https://uc616656e52d68dcc7d1eaaa9f8b.dl.dropboxusercontent.com/cd/0/inline/BnYTpFMd6Uri7bT5F7B-znTiC4Idx1fHbVGk85TMUZ1mLmg8_X7N-7nRGLc1fv2kXc0Hrd3Hx2DwlWIVjIc4UaHNC7ihJ4Eoy-mRcul90k87s6UvWLPVC_P4ElgUu4xU4St6Z4FDOhOyc_EQO0M6UkKs3Bg-slSMOTyN3P55FqkUPG2kOLWUNCGck0_Umf-saUY/file\n",
            "Resolving uc616656e52d68dcc7d1eaaa9f8b.dl.dropboxusercontent.com (uc616656e52d68dcc7d1eaaa9f8b.dl.dropboxusercontent.com)... 162.125.8.15, 2620:100:601b:15::a27d:80f\n",
            "Connecting to uc616656e52d68dcc7d1eaaa9f8b.dl.dropboxusercontent.com (uc616656e52d68dcc7d1eaaa9f8b.dl.dropboxusercontent.com)|162.125.8.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/BnYTYwTebA43NNlyPLq6o9UnHf6IcL35oEd84Rees-yhqeU0rHLqnco7l5wmbJO-1eMWUnyushULT6f-HSSp_Ll604IY4qrGC11p5VVvFs__MQH8H2FpKA9tBI38HyjZXQe8_RYZBs2pP4mwRJFnWWSCbmIlXcAYePc2ib2Pp-KoHg1zNOyfcs2rSuO12SNonqv3bCdmVCuzclI9WsLRP_M1_prEYS8QNq0b1_KU6OtjX-zMn_YUj6DU091AS1-OMjW-6se_9QzFolIMYl8TiG4TkKHmvNpeKV6TTHkdImnsL8GQzSeLWRChAdcoAhNKr9iNWVnDr_39B9c4iHXSH5ORnlcHmGippar4_3c5gywhe_I-HbgGNMhZyp_MC2uar-s-mpkqLdoFPbk0pIyx3CAMKP7sTi5c9iL1JW3bEFxmVCd6tfLBioKhm9E31LlPDeQ/file [following]\n",
            "--2022-06-17 15:30:40--  https://uc616656e52d68dcc7d1eaaa9f8b.dl.dropboxusercontent.com/cd/0/inline2/BnYTYwTebA43NNlyPLq6o9UnHf6IcL35oEd84Rees-yhqeU0rHLqnco7l5wmbJO-1eMWUnyushULT6f-HSSp_Ll604IY4qrGC11p5VVvFs__MQH8H2FpKA9tBI38HyjZXQe8_RYZBs2pP4mwRJFnWWSCbmIlXcAYePc2ib2Pp-KoHg1zNOyfcs2rSuO12SNonqv3bCdmVCuzclI9WsLRP_M1_prEYS8QNq0b1_KU6OtjX-zMn_YUj6DU091AS1-OMjW-6se_9QzFolIMYl8TiG4TkKHmvNpeKV6TTHkdImnsL8GQzSeLWRChAdcoAhNKr9iNWVnDr_39B9c4iHXSH5ORnlcHmGippar4_3c5gywhe_I-HbgGNMhZyp_MC2uar-s-mpkqLdoFPbk0pIyx3CAMKP7sTi5c9iL1JW3bEFxmVCd6tfLBioKhm9E31LlPDeQ/file\n",
            "Reusing existing connection to uc616656e52d68dcc7d1eaaa9f8b.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 45737 (45K) [application/x-tar]\n",
            "Saving to: ‘Corpus.tar’\n",
            "\n",
            "Corpus.tar          100%[===================>]  44.67K  --.-KB/s    in 0.02s   \n",
            "\n",
            "Last-modified header missing -- time-stamps turned off.\n",
            "2022-06-17 15:30:41 (2.35 MB/s) - ‘Corpus.tar’ saved [45737/45737]\n",
            "\n",
            "Corpus/\n",
            "Corpus/._.DS_Store\n",
            "Corpus/.DS_Store\n",
            "Corpus/._DATA\n",
            "Corpus/DATA\n",
            "Corpus/._README\n",
            "Corpus/README\n",
            "name : tweets_opinion_retreival\n",
            "min, mean, max docs len  :  1, 14.9, 38\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame()\n",
        "df['docs'] = docs\n",
        "df['labels'] = labels\n",
        "\n",
        "if data_to_get == 'yahooanswers':\n",
        "  sample_size = 5000\n",
        "  seed = 2022\n",
        "  df = df.groupby('labels').apply(lambda x: x.sample(sample_size,random_state=seed))\n",
        "\n",
        "### save query too\n",
        "if data_to_get == 'opinions_twitter' or data_to_get == 'nfcorpus':\n",
        "  df['queries'] = queries\n",
        "  df['relevance'] = relevance\n",
        "  if data_to_get == 'nfcorpus':\n",
        "    df['doc_id'] = doc_id\n",
        "\n",
        "min_doc_len,min_word_len = 3,3\n",
        "df['docs_preprocessed'] = df['docs'].apply(lambda x: preprocessing(x,word2vec_model,string.punctuation,min_doc_len,min_word_len))\n",
        "df = df.drop_duplicates(subset=['docs_preprocessed'], keep='first').reset_index(drop=True)\n",
        "data_preprocessed = df['docs_preprocessed'].values\n",
        "data_preprocessed_labels = df['labels'].values\n",
        "assert len(data_preprocessed) == len(data_preprocessed_labels)\n",
        "perc_vocab= 0.7\n",
        "vectorizer = CountVectorizer(dtype=np.uint8)\n",
        "train_vec = vectorizer.fit_transform(data_preprocessed).toarray()\n",
        "all_vocab = vectorizer.vocabulary_\n",
        "\n",
        "if data_to_get == 'bbc':\n",
        "  max_features = 2000\n",
        "elif data_to_get == 'nfcorpus':\n",
        "  max_features = 2000\n",
        "elif data_to_get == 'opinions_twitter':\n",
        "  max_features = 2000\n",
        "elif data_to_get == 'searchsnippet':\n",
        "  max_features = 3000\n",
        "elif data_to_get == 'yahooanswers':\n",
        "  max_features = 4000"
      ],
      "metadata": {
        "id": "hw6wtvqMIQJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "\n",
        "# max_features=None\n",
        "os.chdir(home_dir+\"/content\")\n",
        "os.makedirs(home_dir+'/content/data_'+data_to_get+\"/\"+dtype,exist_ok=True)\n",
        "os.chdir(home_dir+'/content/data_'+data_to_get+\"/\"+dtype)\n",
        "\n",
        "with open(data_to_get+'_'+dtype+\".txt\", \"a\") as f:\n",
        "  f.write(data_to_get+\" - \"+dtype)\n",
        "  f.write(\"\\n\\n\")\n",
        "  f.write('**'*50)\n",
        "  f.write(\"\\n\\n\")\n",
        "  data_preprocessed,data_preprocessed_labels,embeddings,vocab = docs_labels_preprocessing(data_preprocessed,data_preprocessed_labels,word2vec_model,min_doc_len,min_word_len,max_features)\n",
        "  f.write('\\n\\nlen of - \\n  data_preprocessed: '+str(len(data_preprocessed))+'\\n  data_preprocessed_labels: '+str(len(data_preprocessed_labels))+'\\n  vocab:  '+str(len(vocab))+'\\n  embeddings : '+str(len(embeddings))+'\\n')\n",
        "  # f.write(str(len(data_preprocessed))+', '+str(len(data_preprocessed_labels))+'\\n')\n",
        "  f.write('(labels,count): '+str(list(zip(*np.unique(data_preprocessed_labels, return_counts=True)))))\n",
        "  len_docs = [len(d.split(\" \")) for d in data_preprocessed]\n",
        "  f.write('\\n min,mean,max docs len: '+str(np.min(len_docs))+', '+str(np.mean(len_docs).round(2))+', '+str(np.max(len_docs))+'\\n\\n')\n",
        "  save_obj(data_preprocessed,'data_preprocessed'+'_'+data_to_get+'_'+dtype)\n",
        "  save_obj(data_preprocessed_labels,'data_preprocessed_labels'+'_'+data_to_get+'_'+dtype)\n",
        "  save_obj(vocab,'vocab'+'_'+data_to_get+'_'+dtype)\n",
        "  save_obj(embeddings,'embeddings'+'_'+data_to_get+'_'+dtype)\n",
        "  save_obj(df,'dataframe_'+data_to_get+'_'+dtype)\n",
        "\n",
        "  ## skipgram embeddings short text\n",
        "  all_tokens = [word_tokenize(t) for t in data_preprocessed]\n",
        "  model = gensim.models.Word2Vec(all_tokens, min_count=0, sg=1, size=300,iter=200, workers=-1)\n",
        "  generated_vocab = list(model.wv.vocab)\n",
        "  generated_embeddings = {}\n",
        "  for v in list(model.wv.vocab):\n",
        "    vec = model.wv.__getitem__(v)\n",
        "    generated_embeddings[v] =vec\n",
        "  save_obj(generated_embeddings,'generated_embeddings_'+data_to_get+\"_\"+dtype)\n",
        "\n",
        "  f.write('**'*50)"
      ],
      "metadata": {
        "id": "-WoUmvIYr4bn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title for sentences\n",
        "# min_doc_len = 1\n",
        "# min_word_len = 3\n",
        "# import gensim\n",
        "# split_to_sentences = True\n",
        "# docs = df['docs'].values\n",
        "# labels = df['labels'].values\n",
        "# if split_to_sentences:\n",
        "#   docs_sentences = [sent_tokenize(d) for d in docs]\n",
        "#   allsent_docs = []\n",
        "#   allsent_labels = []\n",
        "#   for i in range(len(docs_sentences)):\n",
        "#     # print(i)\n",
        "#     data_preprocessed_sents,data_preprocessed_labels_sents,google_embeddings,vocab = docs_labels_preprocessing_sent(\n",
        "#           docs_sentences[i],len(docs_sentences[i])*[labels[i]],word2vec_model,min_doc_len\n",
        "#           ,min_word_len,max_features,split_to_sentences)\n",
        "\n",
        "\n",
        "#     allsent_docs.append(data_preprocessed_sents)\n",
        "#     allsent_labels.append(data_preprocessed_labels_sents)\n",
        "#   allsent_docs = list(filter(None, allsent_docs))\n",
        "#   allsent_labels = list(filter(None, allsent_labels))\n",
        "\n",
        "#   df_sent = pd.DataFrame()\n",
        "#   df_sent['allsent_docs'] = allsent_docs\n",
        "#   df_sent['allsent_labels'] = allsent_labels\n",
        "\n",
        "#   df_sent['allsent_docs_joined'] = df_sent['allsent_docs'].apply(lambda x: ' '.join(l for l in x).strip())\n",
        "#   df_sent['allsent_labels_joined'] = df_sent['allsent_labels'].apply(lambda x: ' '.join(l for l in x).strip())\n",
        "#   df_sent = df_sent.drop_duplicates(subset=['allsent_docs_joined'], keep='first').reset_index(drop=True)\n",
        "\n",
        "#   allsent_docs = df_sent['allsent_docs'].values\n",
        "#   allsent_labels = df_sent['allsent_labels'].values\n",
        "#   all_sents = df_sent['allsent_docs_joined'].values\n",
        "\n",
        "#   # all_sents = flatten(allsent_docs)\n",
        "#   all_docs_sent_tokens = [word_tokenize(t) for t in all_sents]\n",
        "#   model = gensim.models.Word2Vec(all_docs_sent_tokens, min_count=0, sg=1, size=300,iter=200,window=5, workers=-1)\n",
        "#   generated_vocab = list(model.wv.vocab)\n",
        "#   save_obj(df_sent,'sents_dataframe'+data_to_get+\"_\"+dtype)\n",
        "#   save_obj(all_sents,'sents_allsents'+data_to_get+\"_\"+dtype)\n",
        "#   save_obj(allsent_docs,'sents_allsent_docs_'+data_to_get+\"_\"+dtype)\n",
        "#   save_obj(allsent_labels,'sents_allsent_labels_'+data_to_get+\"_\"+dtype)\n",
        "#   generated_embeddings = {}\n",
        "#   for v in list(model.wv.vocab):\n",
        "#     vec = model.wv.__getitem__(v)\n",
        "#     generated_embeddings[v] =vec\n",
        "\n",
        "#   save_obj(google_embeddings,'sents_google_embeddings_'+data_to_get+\"_\"+dtype)\n",
        "#   save_obj(vocab,'sents_countVec_vocab_'+data_to_get+\"_\"+dtype)\n",
        "#   save_obj(generated_embeddings,'sents_generated_embeddings_'+data_to_get+\"_\"+dtype)\n",
        "#   save_obj(generated_vocab,'sents_sg_generated_vocab_'+data_to_get+\"_\"+dtype)"
      ],
      "metadata": {
        "id": "EDrhsFXQDaBN",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# def DESM_score_Corpus(query_list, train_vec, vocab, embeddings):\n",
        "#   num_docs = train_vec.shape[0]\n",
        "#   num_voc = len(vocab)\n",
        "#   sim_list = torch.zeros(num_docs)\n",
        "#   index = 0\n",
        "#   id_vocab = dict(map(reversed, vocab.items()))\n",
        "#   for d in range(num_docs):\n",
        "#     # if(d%5000==0): print(d)\n",
        "#     doc_bar = torch.zeros(300)\n",
        "#     doc_length = 0\n",
        "#     for v in range(num_voc):\n",
        "#       if(train_vec[d][v] > 0):\n",
        "#         doc_bar.add_(train_vec[d][v] * torch.from_numpy(embeddings[id_vocab[v]])/torch.norm(torch.from_numpy(embeddings[id_vocab[v]])))\n",
        "#         doc_length = doc_length + train_vec[d][v]\n",
        "#     doc_bar = doc_bar / doc_length\n",
        "#     sum = 0\n",
        "\n",
        "#     for q in query_list:\n",
        "#       sum += torch.dot(torch.from_numpy(embeddings[q]) , doc_bar)/(torch.norm(torch.from_numpy(embeddings[q]))*torch.norm(doc_bar))\n",
        "#     sum = sum/len(query_list)\n",
        "#     sim_list[index]=sum\n",
        "#     index = index + 1\n",
        "\n",
        "#   return sim_list\n",
        "\n",
        "# # keywords = ['']\n",
        "# # desm_score = DESM_score_Corpus(keywords, train_vec, vocab, embeddings)\n",
        "# # sorted_desm_idx = torch.sort(desm_score,descending=True).indices"
      ],
      "metadata": {
        "id": "_Nrk93X0P34b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d_data = 'yahooanswers'\n",
        "import numpy as np\n",
        "\n",
        "import torch.nn as nn\n",
        "def get_embedding_tensor(word_list,embeddings): return torch.tensor([embeddings[w] for w in word_list])\n",
        "\n",
        "cos_sim = nn.CosineSimilarity(dim=-1, eps=1e-8)\n",
        "\n",
        "\n",
        "def get_embedding_tensor(word_list,embeddings): return torch.tensor([embeddings[w] for w in word_list])\n",
        "\n",
        "def get_all_keywords_score(keywords,word_list,embeddings):\n",
        "    all_cosine_sim = []\n",
        "    for k in keywords:\n",
        "      keyword_embedding = torch.tensor(embeddings[k])\n",
        "      word_embedding = get_embedding_tensor(word_list,embeddings)\n",
        "      keyword_embedding = keyword_embedding.unsqueeze(0).expand(word_embedding.shape)\n",
        "      cosine_sim_score = cos_sim(keyword_embedding,word_embedding)\n",
        "      all_cosine_sim.append(cosine_sim_score)\n",
        "    all_keywords_score = torch.stack(all_cosine_sim)\n",
        "    return all_keywords_score\n",
        "\n",
        "\n",
        "def flatten_list(user_list): return [item for sublist in user_list for item in sublist]\n",
        "def toT(a): return torch.tensor(a)\n",
        "\n",
        "def list_of_tensors_to_tensor(loT):\n",
        "  stacked_tensor = torch.stack(loT)\n",
        "  return stacked_tensor\n",
        "\n",
        "def get_extended_keywords_list(new_data,keywords,embeddings,avg_doc_len):\n",
        "  vec= CountVectorizer(min_df=0,dtype=np.uint8)\n",
        "  vecdata = vec.fit_transform(new_data).toarray()\n",
        "  vec_vocab = vec.vocabulary_\n",
        "\n",
        "  # keywords = query\n",
        "  word_list = sorted(vec_vocab)\n",
        "  all_keywords_score = get_all_keywords_score(keywords,word_list,embeddings)\n",
        "  most_similar_word_dict = {}\n",
        "  extended_keywords_list = []\n",
        "  # extend_each_by = int(avg_doc_len[d_data])\n",
        "  extend_each_by  = 10\n",
        "  for k in range(len(keywords)):\n",
        "    v, i = torch.sort(all_keywords_score[k],descending=True)\n",
        "    most_similar = np.array(word_list)[np.array(i)][:extend_each_by]\n",
        "    extended_keywords_list.append(most_similar)\n",
        "    most_similar_word_dict[keywords[k]] = most_similar\n",
        "  return extend_each_by,extended_keywords_list\n",
        "\n",
        "\n",
        "import torch\n",
        "def DESM_score_Corpus2(query_list, train_vec, vocab, embeddings):\n",
        "  sim_list = torch.zeros(train_vec.shape[0])\n",
        "  index = 0\n",
        "  word_list = np.asarray(sorted(vocab))\n",
        "  for d in train_vec:\n",
        "    words_in_d = np.where(d>0)[0]\n",
        "    all_words_tensor = get_embedding_tensor(np.repeat(word_list[words_in_d],d[words_in_d]),embeddings)\n",
        "    doc_bar = (all_words_tensor/torch.norm(all_words_tensor,dim=1).unsqueeze(-1)).sum(0) / d.sum(0)\n",
        "    D_bar = doc_bar.unsqueeze(0).expand(len(query_list),doc_bar.shape[0])\n",
        "    q = get_embedding_tensor(query_list,embeddings)\n",
        "    norm_div = torch.norm(q,dim=1) * torch.norm(D_bar,dim=1)\n",
        "    sim_list[index]=(torch.mm(q,D_bar.T)[:,0]/norm_div).sum()/len(query_list)\n",
        "    index +=1\n",
        "  return sim_list\n",
        "\n",
        "avg_doc_len = {'bbc':13.8,\n",
        "              'searchsnippet':13.16,\n",
        "              'yahooanswers':11,\n",
        "              'nfcorpus':7.72,\n",
        "              'opinions_twitter':8.69}\n",
        "\n",
        "# for d in set(data_preprocessed_labels):\n",
        "#   embeddings[d]"
      ],
      "metadata": {
        "id": "FXE4_PE5QWGD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for s in set(data_preprocessed_labels):\n",
        "  print(s.split('&'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VyQJq4TFzAyk",
        "outputId": "f5753af3-f936-409e-dec8-f766ab64d68d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['tweets']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "vectorizer = CountVectorizer(min_df=0,dtype=np.uint8)\n",
        "train_vec = vectorizer.fit_transform(data_preprocessed).toarray()\n",
        "keywords = ['business','finance','science','mathematics','society','culture','entertainment','music','sport','health','computer','internet','education','reference']\n",
        "# get_extended_keywords_list(data_preprocessed,keywords,embeddings,avg_doc_len)"
      ],
      "metadata": {
        "id": "L1OHk7d4S-nK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(keywords)):\n",
        "  for v in get_extended_keywords_list(data_preprocessed,keywords,embeddings,avg_doc_len)[1][i]:\n",
        "    desm_score = DESM_score_Corpus2([v],train_vec,vocab,embeddings)\n",
        "    scaler = MinMaxScaler()\n",
        "    norm_desm_score = scaler.fit_transform(desm_score.reshape(-1,1)).reshape(-1)\n",
        "    threshold = 0.6\n",
        "    print(v,len(np.where(norm_desm_score > threshold)[0]))"
      ],
      "metadata": {
        "id": "JnV9ykW3SrQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "desm_score = DESM_score_Corpus2(['industrial'],train_vec,vocab,embeddings)\n",
        "scaler = MinMaxScaler()\n",
        "norm_desm_score = scaler.fit_transform(desm_score.reshape(-1,1)).reshape(-1)\n",
        "threshold = 0.6\n",
        "print(len(np.where(norm_desm_score > threshold)[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eft58s2wast2",
        "outputId": "0b861dee-8557-4baa-cc26-6ba6250c4ef8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "146\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "desm_score = DESM_score_Corpus(['engineering', 'engineer', 'design'],train_vec,vocab,embeddings)"
      ],
      "metadata": {
        "id": "YhIpBL9gxiv3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "desm_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XY171oXnlDw8",
        "outputId": "98937a06-fc6b-4dda-9d0f-a2cdbb1ccfd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.2754, 0.4099, 0.3838,  ..., 0.1152, 0.0825, 0.1188])"
            ]
          },
          "metadata": {},
          "execution_count": 311
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "desm_score = DESM_score_Corpus2(['engineering', 'engineer', 'design'],train_vec,vocab,embeddings)"
      ],
      "metadata": {
        "id": "0IBNpj2J15nK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "desm_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aaQPBXdt17CK",
        "outputId": "3991cbdf-a5dc-4d74-9a91-c1d6ef1b9632"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.2754, 0.4099, 0.3838,  ..., 0.1152, 0.0825, 0.1188])"
            ]
          },
          "metadata": {},
          "execution_count": 316
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# keywords = list(set(data_preprocessed_labels))\n",
        "# keywords = ['engineering','sport','computer','business','health']\n",
        "keywords = ['business','finance','science','mathematics','society','culture','entertainment','music','sport','health','computer','internet','education','reference']\n",
        "get_extended_keywords_list(data_preprocessed,keywords,embeddings,avg_doc_len)"
      ],
      "metadata": {
        "id": "7YKQK8reU0db"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # # (popular, rare)\n",
        "# # bbc_queries = [['film','piracy'], # (rare, rare)  (68,11) # entertainment\n",
        "# #                ['chat','celebrity'], # (popular, rare)  (336,13) # entertainment\n",
        "# #                ['hollywood','industry'] # (popular, popular)  (159,136) # entertainment\n",
        "# #                ,\n",
        "# #                ['private','security'],  # (popular, rare) (242,40) # tech\n",
        "# #                ['corporate','sector'],  # (popular, popular) (247,266) # tech\n",
        "# #                ['digital','art'] # (rare, rare) (53,9) # tech\n",
        "# #                ,\n",
        "# #                  ['bank','mortgage'], # (rare, rare) (33,42) # business\n",
        "# #                  ['private','property'], # (popular, rare) (242,19) # business\n",
        "# #                  ['global','operation'] # (popular, popular) (252,126) # business\n",
        "# #                  ,\n",
        "# #                  ['young','talent'], # (rare, popular)(41,122) # sport\n",
        "# #                  ['race','sprinter'], # (rare, rare)(13 ,14) # sport\n",
        "# #                  ['olympics','champion'] # (popular, popular)(336,155) # sport\n",
        "# #                  ,\n",
        "# #                  ['immigration','law'], # (rare,rare) (14,25) # politics\n",
        "# #                  ['civil','administration'], # (rare,popular) (50,271) # politics\n",
        "# #                  ['government','administration'] # (popular,popular) (195,271) # policy (policy-119)\n",
        "# #                  ]\n",
        "\n",
        "\n",
        "# ['sector','corporate'] # pp\n",
        "# ['law','immigration'] # rr\n",
        "# ['private','bank','debt'] # prr\n",
        "# ['administration','policy','public'] # ppr"
      ],
      "metadata": {
        "id": "GNr6eCVkRFEL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0a5fce0-3288-4e35-c913-f2c47c511e29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['administration', 'policy', 'public']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "allwords_inCorpus = []\n",
        "for d in data_preprocessed:\n",
        "  allwords_inCorpus.extend(word_tokenize(d))\n",
        "allword_Count = Counter(allwords_inCorpus)\n",
        "sorted(allword_Count)\n",
        "allword_Count = dict(sorted(allword_Count.items(), key=lambda item: item[1], reverse=True))\n"
      ],
      "metadata": {
        "id": "mIhnwA5lXyKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# os.chdir(home_dir)\n",
        "# # data_to_get = 'yahooanswers'\n",
        "# # max_features = 'all_vocab'\n",
        "# zipped_pickle_filename = data_to_get+\"_\"+dtype\n",
        "# os.system('zip -r '+zipped_pickle_filename+'_'+str(max_features)+'_.zip '+'./content/data_'+data_to_get)\n",
        "# # files.download(zipped_pickle_filename+'_'+str(max_features)+'_.zip')"
      ],
      "metadata": {
        "id": "9tBHfShEs0GZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "STOP!!"
      ],
      "metadata": {
        "id": "jXbe6MWOhe8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(home_dir)"
      ],
      "metadata": {
        "id": "yuCjWUkOZeTc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# queryset = 1\n",
        "# frac = 0.05\n",
        "# os.chdir(home_dir+'/content/data_'+d_data+\"/\"+dtype+'/injected_docs/'+str(frac))\n",
        "# files = os.listdir('.')\n",
        "# data_dict = decompress_pickle(files[queryset-1].split('.')[0])\n",
        "# data_preprocessed = data_dict['new_docs']\n",
        "# data_preprocessed_labels = data_dict['new_labels']\n",
        "# ground_truth_docs = data_dict['docs_injected']\n",
        "# ground_truth_labels = data_dict['labels_injected']\n",
        "# keywords = data_dict['query']\n",
        "# extended_keywords_list = data_dict['extended_keywords_list']"
      ],
      "metadata": {
        "id": "9OZeFmBTqfgM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data =data_preprocessed\n",
        "# labels = data_preprocessed_labels\n",
        "# data = list(data)\n",
        "# print('Before Injection : ',len(data),len(labels))\n",
        "\n",
        "# keywords_as_labels = []\n",
        "# extended_keywords_list_joined = np.asarray([list(extended_keywords_list[x]) + (list(extended_keywords_list[x+1])) for x in range(len(extended_keywords_list)) if x%2==0])\n",
        "# keywords_as_labels = np.asarray(['p{'+keywords[x]+'&'+keywords[x+1]+'}' for x in range(len(keywords)) if x%2==0] )\n",
        "# for e in range(len(extended_keywords_list_joined)):\n",
        "#   data.append(' '.join(extended_keywords_list_joined[e]))\n",
        "#   labels = np.append(labels,keywords_as_labels[e])\n",
        "# print('After Injection : ',len(data),len(labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T5s4YoiJ5aZ0",
        "outputId": "58456b95-84c3-4c11-fe32-01a179e61d20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before Injection :  37238 37238\n",
            "After Injection :  37240 37240\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNBWvIY5itSO"
      },
      "source": [
        "## injection / query selection"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "d_data = 'opinions_twitter'\n",
        "dtype = 'short'\n",
        "os.chdir(home_dir+'/content/data_'+d_data+\"/\"+dtype)\n",
        "data_preprocessed = load_obj_pkl5('data_preprocessed_'+d_data+'_'+dtype)\n",
        "data_preprocessed_labels = load_obj_pkl5('data_preprocessed_labels_'+d_data+'_'+dtype)\n",
        "df = load_obj_pkl5('dataframe_'+d_data+'_'+dtype)\n",
        "embeddings = load_obj_pkl5('embeddings_'+d_data+'_'+dtype)\n",
        "vocab = load_obj_pkl5('vocab_'+d_data+'_'+dtype)\n",
        "queries_data_dict = decompress_pickle('queries_'+d_data)"
      ],
      "metadata": {
        "id": "xG-iuIHinVjm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "queries_data_dict"
      ],
      "metadata": {
        "id": "42YYk0NDzL4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(data_preprocessed),len(embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3GK9s-GqUAbT",
        "outputId": "d54c3bb7-c89b-4586-d915-9302d3b32b3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2259, 2000)"
            ]
          },
          "metadata": {},
          "execution_count": 146
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ICDM 2022 query and injecton class selection\n",
        "\n",
        "# from collections import Counter\n",
        "# if d_data !='nfcorpus' or d_data !='opinions_twitter':\n",
        "#   d = Counter(data_preprocessed_labels)\n",
        "#   asc_sorted_classes = sorted(d,key=d.get)\n",
        "#   least_2_classes = asc_sorted_classes[:2]\n",
        "#   least_3_classes = asc_sorted_classes[:3]\n",
        "#   d,least_2_classes,least_3_classes\n",
        "# print(least_2_classes,least_3_classes)\n",
        "\n",
        "\n",
        "### bbc\n",
        "\n",
        "# query = ['technology','entertainment']\n",
        "# injection_class = least_2_classes\n",
        "\n",
        "# query = ['technology','entertainment','political']\n",
        "# injection_class = least_3_classes\n",
        "\n",
        "### searchsnippet\n",
        "\n",
        "# query = ['engineering','health']\n",
        "# injection_class = least_2_classes\n",
        "\n",
        "# query = ['engineering','health','sport']\n",
        "# injection_class = least_3_classes\n",
        "\n",
        "### yahooanswers\n",
        "# query = ['science','mathematics','business','finance']\n",
        "# injection_class = least_2_classes\n",
        "\n",
        "# query = ['science','mathematics','business','finance','education','reference']\n",
        "# injection_class = least_3_classes\n",
        "\n",
        "### nfcorpus\n",
        "# query = ['dietary', 'intervention', 'woman']\n",
        "# injection_class = 'what does the research say about dietary interventions on women with pcos ?'\n",
        "\n",
        "# query = ['coffee', 'mortality']\n",
        "# injection_class = 'coffee and mortality'\n",
        "\n",
        "### opinions_twitter\n",
        "# query = ['job','hunting']\n",
        "# injection_class = 'job hunting'\n",
        "\n",
        "# query = ['kinect']\n",
        "# injection_class = 'kinect'"
      ],
      "metadata": {
        "id": "nzO2WqSjHdxp",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from collections import Counter\n",
        "# allwords_inCorpus = []\n",
        "# for d in data_preprocessed:\n",
        "#   allwords_inCorpus.extend(word_tokenize(d))\n",
        "# allword_Count = Counter(allwords_inCorpus)\n",
        "# sorted(allword_Count)\n",
        "# allword_Count = dict(sorted(allword_Count.items(), key=lambda item: item[1], reverse=True))\n",
        "\n",
        "# words= list(vocab.keys())\n",
        "# import random\n",
        "# queries = []\n",
        "# for i in range(5):\n",
        "#   queries.append(random.choices(words,k=2))\n",
        "\n",
        "bbc_queries = [\n",
        "['sector','corporate'],# pp\n",
        "['law','immigration'], # rr\n",
        "['administration','policy','public'], # ppr\n",
        "['private','bank','debt']] # prr\n",
        "\n",
        "searchsnippet_queries = [\n",
        "['biz','economics'], #  pp (1437,612)\n",
        "['healthcare','fitness'], # rr (48,36)\n",
        "['sporting','athlete','racing'], # ppr (780,690,63)\n",
        "['notebook','microprocessor','disk']] # prr (549,79,46)\n",
        "\n",
        "yahooanswers_queries = [\n",
        "['lifestyle','school'], #pp\n",
        "['disease','diabetes'], # rr\n",
        "['musical','song','guitar'], # ppr\n",
        "['youtube','social','gaming'] #prr\n",
        "]\n",
        "\n",
        "#\n",
        "\n",
        "#['treating','dry','eye','disease','diet','water'],\n",
        "#['living', 'longer', 'reducing', 'leucine', 'intake'],\n",
        "#['rye','bread','protect','cancer'],\n",
        "#['fish' ,'consumption', 'suicide']\n",
        "#]\n",
        "#\n",
        "\n",
        "nfcorpus_model_queries = [\n",
        "['dietary', 'intervention', 'woman'],\n",
        "['coffee','mortality'],\n",
        "['bowel','movement'],\n",
        "['diabetes'],\n",
        "['eye','disease'],\n",
        "['living','longer','leucine'],\n",
        "['rye','bread','cancer'],\n",
        "['fish' ,'consumption', 'suicide']\n",
        "]\n",
        "\n",
        "\n",
        "nfcorpus_whole_queries = ['what does the research say about dietary interventions on women with pcos ?',\n",
        "                          'coffee and mortality',\n",
        "                          'how many bowel movements should you have every day ?',\n",
        "                          'what causes diabetes ?',\n",
        "                          'treating dry eye disease with diet : just add water ?',\n",
        "                          'living longer by reducing leucine intake',\n",
        "                          'does rye bread protect against cancer ?',\n",
        "                          'fish consumption and suicide']\n",
        "\n",
        "opinions_twitter_queries = [\n",
        "['job','hunting'],\n",
        "['kinect'],\n",
        "['htc'],\n",
        "['speech','recognition'],\n",
        "['iran','nuclear'],\n",
        "['obama'],\n",
        "['manchester'],\n",
        "['pixar']]\n",
        "\n",
        "opinions_twitter_whole_queries = ['job hunting','kinect','htc','speech recognition','iran nuclear','Obama','manchester city','pixar']\n",
        "\n",
        "if d_data =='bbc': queries = bbc_queries\n",
        "elif d_data =='searchsnippet': queries = searchsnippet_queries\n",
        "elif d_data =='yahooanswers': queries = yahooanswers_queries\n",
        "elif d_data =='nfcorpus': queries = nfcorpus_model_queries\n",
        "elif d_data =='opinions_twitter': queries = opinions_twitter_queries\n",
        "else: queries = []\n",
        "\n",
        "avg_doc_len = {'bbc':12.01,\n",
        "              'searchsnippet':13.16,\n",
        "              'yahooanswers':11,\n",
        "              'nfcorpus':7.72,\n",
        "              'opinions_twitter':7.15\n",
        "              #  'opinions_twitter':12\n",
        "               }\n",
        "\n",
        "# avg_doc_len = {'bbc':3,\n",
        "#               'searchsnippet':3,\n",
        "#               'yahooanswers':3,\n",
        "#               'nfcorpus':3,\n",
        "#                'opinions_twitter':3}"
      ],
      "metadata": {
        "id": "MxmfBbqIpfdp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ss = (embeddings['bowel'] + embeddings['movement'])"
      ],
      "metadata": {
        "id": "073EwKlfhUBm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# job + hunting: ['hunting', 'job', 'work', 'employment', 'doe', 'turkey', 'muskrat']\n",
        "# speech + recognition: ['speech', 'recognition', 'presentation', 'lecture', 'keynote','award', 'message']\n",
        "# bowel + movement: ['bowel', 'movement', 'bladder', 'intestine', 'colon', 'gastric','esophagus']"
      ],
      "metadata": {
        "id": "2Ms3PSZlhtJ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for query in queries:\n",
        "  for q in query: embeddings[q]"
      ],
      "metadata": {
        "id": "_L4Hod_zSvAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch\n",
        "import random\n",
        "\n",
        "seed = 2022\n",
        "random.seed(seed)\n",
        "np.random.seed(seed=seed)\n",
        "\n",
        "\n",
        "def get_docs_idx_in_vis(relv_docs,preprossed_data_non_zeros,doc_ids):\n",
        "  d = {item: idx for idx, item in enumerate(preprossed_data_non_zeros)}\n",
        "  doc_ids_list = list(doc_ids)\n",
        "  relv_docs_idx = [d.get(item) for item in relv_docs]\n",
        "  docs_in_vis_idx = [doc_ids_list.index(r_i) for r_i in relv_docs_idx]\n",
        "  assert (doc_ids[docs_in_vis_idx] == relv_docs_idx).all() == True\n",
        "  return docs_in_vis_idx\n",
        "\n",
        "def vocab_filtered_data(doc,vocab):\n",
        "  doc = word_tokenize(doc)\n",
        "  doc = filter(lambda x: x in vocab, doc)\n",
        "  doc = ' '.join(e for e in doc)\n",
        "  return doc\n",
        "\n",
        "### IR Datasets\n",
        "# df['docs_preprocessed_vocab_filtered'] = df['docs_preprocessed'].apply(lambda x: vocab_filtered_data(x,vocab))\n",
        "# df = df[df['docs_preprocessed_vocab_filtered'].isin(data_preprocessed)].reset_index(drop=True)\n",
        "# ground_truth_docs = df[df['queries']==injection_class[0]]['docs_preprocessed_vocab_filtered'].values\n",
        "\n",
        "# d = {item: idx for idx, item in enumerate(data_preprocessed)}\n",
        "# relv_docs_idx = [d.get(item) for item in ground_truth_docs]\n",
        "\n",
        "def injection(query,injection_class,frac,data_preprocessed,data_preprocessed_labels):\n",
        "  all_injected_docs = []\n",
        "  all_injected_labels = []\n",
        "  all_txt_inj = []\n",
        "  d = Counter(data_preprocessed_labels)\n",
        "  # injection_class = sorted(d, key=d.get)[:2]\n",
        "  # injection_class = ['Sports','Health']\n",
        "  np_data = np.asarray(data_preprocessed)\n",
        "  np_labels = np.asarray(data_preprocessed_labels)\n",
        "\n",
        "  inject_to =  ~np.isin(np_labels, injection_class)\n",
        "  inject_to_docs = np_data[inject_to]\n",
        "  inject_to_labels = np_labels[inject_to]\n",
        "\n",
        "  p=frac\n",
        "\n",
        "  for i in range(len(injection_class)):\n",
        "    to_inject= np.where(np_labels == injection_class[i])[0]\n",
        "    rand_perm_k_to_inject = np.random.RandomState(seed=seed).permutation(to_inject)\n",
        "    to_inject_docs = np_data[rand_perm_k_to_inject]\n",
        "    take_docs_to_inject = to_inject_docs[:round(len(to_inject_docs)*p)]\n",
        "    all_txt_inj.append((str(frac)+\" \"+str(len(take_docs_to_inject))+' docs injected of '+injection_class[i]))\n",
        "    all_injected_docs.append(take_docs_to_inject)\n",
        "    take_labels_to_inject = len(take_docs_to_inject)*['injected_'+injection_class[i]]\n",
        "    all_injected_labels.append(take_labels_to_inject)\n",
        "    inject_to_docs = np.asarray(list(inject_to_docs) + list(take_docs_to_inject))\n",
        "    inject_to_labels = np.asarray(list(inject_to_labels) + list(take_labels_to_inject))\n",
        "\n",
        "  # sorted_unique_labels = sorted(set(inject_to_labels))\n",
        "  print(all_txt_inj)\n",
        "  new_data = inject_to_docs\n",
        "  new_labels = inject_to_labels\n",
        "  assert  len(new_data) == len(new_labels)\n",
        "  return new_data,new_labels,all_injected_docs,all_injected_labels,all_txt_inj\n",
        "\n",
        "cos_sim = nn.CosineSimilarity(dim=-1, eps=1e-8)\n",
        "\n",
        "def get_embedding_tensor(word_list,embeddings): return torch.tensor([embeddings[w] for w in word_list])\n",
        "\n",
        "def get_all_keywords_score(keywords,word_list,embeddings):\n",
        "    all_cosine_sim = []\n",
        "    for k in keywords:\n",
        "      keyword_embedding = torch.tensor(embeddings[k])\n",
        "      word_embedding = get_embedding_tensor(word_list,embeddings)\n",
        "      keyword_embedding = keyword_embedding.unsqueeze(0).expand(word_embedding.shape)\n",
        "      cosine_sim_score = cos_sim(keyword_embedding,word_embedding)\n",
        "      all_cosine_sim.append(cosine_sim_score)\n",
        "    all_keywords_score = torch.stack(all_cosine_sim)\n",
        "    return all_keywords_score\n",
        "\n",
        "def get_extended_keywords_list(new_data,keywords,embeddings,avg_doc_len,th,use_th):\n",
        "  vec= CountVectorizer(min_df=0,dtype=np.uint8)\n",
        "  vecdata = vec.fit_transform(new_data).toarray()\n",
        "  vec_vocab = vec.vocabulary_\n",
        "\n",
        "  word_list = sorted(vec_vocab)\n",
        "  all_keywords_score = get_all_keywords_score(keywords,word_list,embeddings)\n",
        "  most_similar_word_dict = {}\n",
        "  extended_keywords_list = []\n",
        "  extend_each_by = int(avg_doc_len[d_data])\n",
        "\n",
        "  for k in range(len(keywords)):\n",
        "    v, i = torch.sort(all_keywords_score[k],descending=True)\n",
        "    if not use_th:\n",
        "      most_similar = np.array(word_list)[np.array(i)][:extend_each_by]\n",
        "    else:\n",
        "      th_idx = i[torch.where(v>=th)[0]]\n",
        "      most_similar = np.array(word_list)[np.array(th_idx)]\n",
        "    extended_keywords_list.append(most_similar)\n",
        "    most_similar_word_dict[keywords[k]] = most_similar\n",
        "  return extend_each_by,extended_keywords_list\n",
        "\n",
        "# def f7(seq):\n",
        "#     seen = set()\n",
        "#     seen_add = seen.add\n",
        "#     return [x for x in seq if not (x in seen or seen_add(x))]\n",
        "# def split_list(a_list):\n",
        "#     half = len(a_list)//2\n",
        "#     return a_list[:half], a_list[half:]"
      ],
      "metadata": {
        "id": "i_Iu-AJVN5QW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if d_data == 'bbc' or d_data == 'searchsnippet' or  d_data == 'yahooanswers':\n",
        "  extended_keywords_list_allQ = []\n",
        "  extend_each_by_allQ = []\n",
        "  query_dict = {}\n",
        "  for q in range(len(queries)):\n",
        "    extend_each_by,extended_keywords_list = get_extended_keywords_list(data_preprocessed,queries[q],embeddings,avg_doc_len,th,use_th=False)\n",
        "    # extended_keywords_list_allQ.append(extended_keywords_list)\n",
        "    # extend_each_by_allQ.append(extend_each_by)\n",
        "\n",
        "    data_dict = {}\n",
        "    data_dict['extend_each_by']  = extend_each_by\n",
        "    data_dict['extended_keywords_list'] = extended_keywords_list\n",
        "    data_dict['query'] = queries[q]\n",
        "    data_dict['whole_query'] = queries[q]\n",
        "    query_dict[str(q+1)] = data_dict\n",
        "\n",
        "  os.makedirs(home_dir+'/content/data_'+d_data+\"/\"+dtype,exist_ok=True)\n",
        "  os.chdir(home_dir+'/content/data_'+d_data+\"/\"+dtype)\n",
        "  compressed_pickle(query_dict,'queries_'+d_data)"
      ],
      "metadata": {
        "id": "Q4J2AIWz8FXV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if d_data == 'nfcorpus' or d_data == 'opinions_twitter':\n",
        "  extended_keywords_list_allQ = []\n",
        "  extend_each_by_allQ = []\n",
        "  th = 0.0\n",
        "  query_dict = {}\n",
        "  for q in range(len(queries)):\n",
        "    extend_each_by,extended_keywords_list = get_extended_keywords_list(data_preprocessed,queries[q],embeddings,avg_doc_len,th,use_th=False)\n",
        "\n",
        "    data_dict = {}\n",
        "    data_dict['extend_each_by']  = extend_each_by\n",
        "    data_dict['extended_keywords_list'] = extended_keywords_list\n",
        "    data_dict['query'] = queries[q]\n",
        "    # data_dict['whole_query'] = queries[q]\n",
        "\n",
        "    if d_data == 'nfcorpus' or d_data == 'opinions_twitter':\n",
        "\n",
        "      df['docs_preprocessed_vocab_filtered'] = df['docs_preprocessed'].apply(lambda x: vocab_filtered_data(x,vocab) )\n",
        "      df = df[df['docs_preprocessed_vocab_filtered'].isin(data_preprocessed)].reset_index(drop=True)\n",
        "\n",
        "      if d_data == 'nfcorpus':\n",
        "        df_all_qdocs = df[df['queries']==nfcorpus_whole_queries[q]].reset_index(drop=True)\n",
        "        ground_truth_docs = df_all_qdocs[df_all_qdocs['relevance']==2]['docs_preprocessed_vocab_filtered'].values\n",
        "        ground_truth_labels = 'MED'+df_all_qdocs[df_all_qdocs['relevance']==2]['relevance'].astype('str').values\n",
        "\n",
        "      elif d_data == 'opinions_twitter':\n",
        "        df_all_qdocs = df[df['queries']==opinions_twitter_whole_queries[q]].reset_index(drop=True)\n",
        "        ground_truth_docs = df_all_qdocs[df_all_qdocs['relevance']=='T']['docs_preprocessed_vocab_filtered'].values\n",
        "        ground_truth_labels = df_all_qdocs[df_all_qdocs['relevance']=='T']['relevance'].values\n",
        "\n",
        "      data_dict['ground_truth_docs'] = ground_truth_docs\n",
        "      data_dict['ground_truth_labels'] = ground_truth_labels\n",
        "      # data_dict['df_all_qdocs'] = df_all_qdocs\n",
        "\n",
        "    query_dict[str(q+1)] = data_dict\n",
        "\n",
        "  os.makedirs(home_dir+'/content/data_'+d_data+\"/\"+dtype,exist_ok=True)\n",
        "  os.chdir(home_dir+'/content/data_'+d_data+\"/\"+dtype)\n",
        "  compressed_pickle(query_dict,'queries_'+d_data)"
      ],
      "metadata": {
        "id": "8jbT4UNeh2Lx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qs = '4'"
      ],
      "metadata": {
        "id": "6Zo1DxVKRzBG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keywords = query_dict[qs]['query']\n",
        "whole_query = [' '.join(keywords)]\n",
        "vec= CountVectorizer(min_df=0,dtype=np.uint8)\n",
        "vecdata = vec.fit_transform(data_preprocessed).toarray()\n",
        "vec_vocab = vec.vocabulary_\n",
        "\n",
        "word_list = sorted(vec_vocab)\n",
        "\n",
        "all_cosine_sim = []\n",
        "for k in keywords:\n",
        "  keyword_embedding = torch.tensor(embeddings[k])\n",
        "  word_embedding = get_embedding_tensor(word_list,embeddings)\n",
        "  keyword_embedding = keyword_embedding.unsqueeze(0).expand(word_embedding.shape)\n",
        "  cosine_sim_score = cos_sim(keyword_embedding,word_embedding)\n",
        "  all_cosine_sim.append(cosine_sim_score)\n",
        "\n",
        "most_similar_word_dict = {}\n",
        "extended_keywords_list_together = []\n",
        "extend_each_by = 6\n",
        "\n",
        "cosine_sim_score = list_of_tensors_to_tensor(all_cosine_sim).sum(0)\n",
        "# # for k in range(len(keywords)):\n",
        "v, i = torch.sort(cosine_sim_score,descending=True)\n",
        "# if not use_th:\n",
        "most_similar = np.array(word_list)[np.array(i)][:extend_each_by]\n",
        "#   else:\n",
        "#     th_idx = i[torch.where(v>=th)[0]]\n",
        "#     most_similar = np.array(word_list)[np.array(th_idx)]\n",
        "extended_keywords_list_together.append(most_similar)\n",
        "# most_similar_word_dict[keywords[k]] = most_similar"
      ],
      "metadata": {
        "id": "cLpZe6s1HRBm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extended_keywords_list_together,whole_query"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g87FXR6-HgbM",
        "outputId": "65f06457-5392-47de-caf3-10d552e1372f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([array(['recognition', 'speech', 'presentation', 'award', 'lecture',\n",
              "         'keynote'], dtype='<U14')],\n",
              " ['speech recognition'])"
            ]
          },
          "metadata": {},
          "execution_count": 346
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query_dict[qs]['extended_keywords_list'] = extended_keywords_list_together\n",
        "query_dict[qs]['whole_query'] = whole_query\n",
        "query_dict[qs]['extend_each_by'] = extend_each_by\n",
        "compressed_pickle(query_dict,'queries_'+d_data)"
      ],
      "metadata": {
        "id": "WCVPL5RRlKxo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_dict"
      ],
      "metadata": {
        "id": "MAab_QzPIIke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extended_keywords_list_allQ = []\n",
        "# extend_each_by_allQ = []\n",
        "# th = 0.0\n",
        "# query_dict = {}\n",
        "# for q in range(len(queries)):\n",
        "#   extend_each_by,extended_keywords_list = get_extended_keywords_list(data_preprocessed,queries[q],embeddings,avg_doc_len,th,use_th=True)\n",
        "\n",
        "#   data_dict = {}\n",
        "#   data_dict['extend_each_by']  = extend_each_by\n",
        "#   data_dict['extended_keywords_list'] = extended_keywords_list\n",
        "#   data_dict['query'] = queries[q]\n",
        "#   query_dict[str(q+1)] = data_dict\n",
        "\n",
        "# os.makedirs(home_dir+'/content/data_'+d_data+\"/\"+dtype,exist_ok=True)\n",
        "# os.chdir(home_dir+'/content/data_'+d_data+\"/\"+dtype)\n",
        "# compressed_pickle(query_dict,'queries_'+d_data+'_'+str(th))"
      ],
      "metadata": {
        "id": "np1XFOVSWSAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if d_data == 'opinions_twitter' or d_data == 'nfcorpus':\n",
        "  frac = 0.2\n",
        "  data_dict = {}\n",
        "  df['docs_preprocessed_vocab_filtered'] = df['docs_preprocessed'].apply(lambda x: vocab_filtered_data(x,vocab) )\n",
        "  df = df[df['docs_preprocessed_vocab_filtered'].isin(data_preprocessed)].reset_index(drop=True)\n",
        "\n",
        "  df_all_qdocs = df[df['queries']==injection_class].reset_index(drop=True)\n",
        "  opinions_twitter_ground_truth_docs = df_all_qdocs[df_all_qdocs['relevance']=='T']['docs_preprocessed_vocab_filtered'].values\n",
        "  m\n",
        "  df_all_qdocs = df[df['queries']==injection_class].reset_index(drop=True)\n",
        "  nfcorpus_ground_truth_docs = df_all_qdocs[df_all_qdocs['relevance']==2]['docs_preprocessed_vocab_filtered'].values\n",
        "\n",
        "\n",
        "  # ground_truth_docs = df[df['queries']==injection_class[0]]['docs_preprocessed'].values\n",
        "  nfcorpus_ground_truth_labels = 'MED'+df_all_qdocs[df_all_qdocs['relevance']==2]['relevance'].astype('str').values\n",
        "  opinions_twitter_ground_truth_labels = df_all_qdocs[df_all_qdocs['relevance']=='T']['relevance'].values\n",
        "\n",
        "  extend_each_by,extended_keywords_list = get_extended_keywords_list(data_preprocessed,query,query,embeddings,avg_doc_len)\n",
        "  data_dict['new_docs'] = df['docs_preprocessed_vocab_filtered'].tolist()\n",
        "  data_dict['new_labels'] = df['labels'].tolist()\n",
        "\n",
        "\n",
        "  if d_data == 'opinions_twitter':\n",
        "    data_dict['docs_injected'] = opinions_twitter_ground_truth_docs\n",
        "    data_dict['labels_injected'] = opinions_twitter_ground_truth_labels\n",
        "    data_dict['info']  = str(len(opinions_twitter_ground_truth_docs))+' relevant docs for query '+str(query)\n",
        "\n",
        "  elif d_data == 'nfcorpus':\n",
        "    data_dict['docs_injected'] = nfcorpus_ground_truth_docs\n",
        "    data_dict['labels_injected'] = nfcorpus_ground_truth_labels\n",
        "    data_dict['info']  = str(len(nfcorpus_ground_truth_docs))+' directly relevant docs for query '+str(query)\n",
        "\n",
        "  data_dict['injection_class'] = injection_class\n",
        "  data_dict['extend_each_by']  = extend_each_by\n",
        "  data_dict['extended_keywords_list'] = extended_keywords_list\n",
        "  data_dict['query'] = query\n",
        "\n",
        "  os.makedirs(home_dir+'/content/data_'+d_data+\"/\"+dtype+\"/injected_docs/\"+str(frac),exist_ok=True)\n",
        "\n",
        "  os.chdir(home_dir+'/content/data_'+d_data+\"/\"+dtype+\"/injected_docs/\"+str(frac))\n",
        "  compressed_pickle(data_dict,'injected_'+d_data+\"_\"+'_'.join(query)+\"_injClass_\"+'_'.join(injection_class.split(' ')))"
      ],
      "metadata": {
        "id": "GTV_SDQwWIp5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  all_aspects_all_keywords = flatten_list(extended_keywords_list)\n",
        "  query_as_doc = ' '.join(all_aspects_all_keywords)\n",
        "\n",
        "  #### DESM\n",
        "  desm_score = DESM_score_Corpus(all_aspects_all_keywords, train_vec, vocab, embeddings)\n",
        "  sorted_desm_idx = torch.sort(desm_score,descending=True).indices\n",
        "  ####\n",
        "\n",
        "  #### TF-IDF\n",
        "  tfdifvec = TfidfVectorizer()\n",
        "  tfdifvec.fit(preprossed_data_non_zeros)\n",
        "  tfdif_doc_vectors = torch.from_numpy(tfdifvec.transform(preprossed_data_non_zeros).toarray())\n",
        "  tfdif_query_vectors = torch.from_numpy(tfdifvec.transform([query_as_doc]).toarray())\n",
        "\n",
        "  tfidf_score = cos_sim(tfdif_query_vectors,tfdif_doc_vectors)\n",
        "  sorted_tfidf_idx = torch.sort(tfidf_score,descending=True).indices\n"
      ],
      "metadata": {
        "id": "RAUs0Rx7SAjl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_to_get = 'searchsnippet'\n",
        "paper='_emnlp_2022'\n",
        "os.chdir(home_dir)\n",
        "# max_features = 'all_vocab'\n",
        "zipped_pickle_filename = data_to_get+\"_\"+dtype+paper\n",
        "os.system('zip -r '+zipped_pickle_filename+'_.zip '+'./content/data_'+data_to_get)\n",
        "# files.download(zipped_pickle_filename+'_'+str(max_features)+'_.zip')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zXOOuNmCs8Wq",
        "outputId": "14b9fc28-5948-4afe-d1ff-10b18eb9b842"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMcmf822hTir",
        "outputId": "903b6742-889d-4d99-9828-7266b2857981"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/home/grad16/sakumar/ICDM_Experiments_2022/dataset'"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "STOP!!"
      ],
      "metadata": {
        "id": "mQKuqXkes96i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ICDM 2022 document injection\n",
        "# avg_doc_len = {'bbc':13.8,\n",
        "#               'searchsnippet':14.87,\n",
        "#               'yahooanswers':11.82,\n",
        "#               'nfcorpus':8.19,\n",
        "#               'opinions_twitter':8.69}\n",
        "\n",
        "# if d_data == 'opinions_twitter' or d_data == 'nfcorpus':\n",
        "#   pass\n",
        "# else:\n",
        "#   fracs = [0.05,0.1,0.15,0.2]\n",
        "#   # fracs = [0.15]\n",
        "#   for frac in fracs:\n",
        "#     new_data , new_labels, all_injected_docs,all_injected_labels,all_txt_inj = injection(query,injection_class,frac,data_preprocessed,data_preprocessed_labels)\n",
        "#     extend_each_by,extended_keywords_list = get_extended_keywords_list(new_data,query,query,embeddings,avg_doc_len)\n",
        "\n",
        "#     if d_data == \"yahooanswers\":\n",
        "#       extended_keywords_list_o = extended_keywords_list[:]\n",
        "#       extended_keywords_list_joined = np.asarray([list(extended_keywords_list_o[x]) + (list(extended_keywords_list_o[x+1])) for x in range(len(extended_keywords_list_o)) if x%2==0])\n",
        "\n",
        "#       # extend_each_by = int(extend_each_by/2)\n",
        "#       extended_keywords_list_oo =[]\n",
        "#       for ex in range(len(extended_keywords_list_o)):\n",
        "#         ext = extended_keywords_list_o[ex]\n",
        "#         extended_keywords_list_oo0 = []\n",
        "#         for e in range(len(ext)):\n",
        "#           if e!=0 and ext[e] in extended_keywords_list_joined:\n",
        "#             pass\n",
        "#           if ex<len(extended_keywords_list_o)-1:\n",
        "#             if ext[e] in extended_keywords_list_o[ex+1]:\n",
        "#               pass\n",
        "#             else:\n",
        "#               extended_keywords_list_oo0.append(ext[e])\n",
        "#         extended_keywords_list_oo0.extend(extended_keywords_list_o[ex][:int(extend_each_by/2)])\n",
        "#         extended_keywords_list_oo.append(np.asarray(extended_keywords_list_oo0[:int(extend_each_by/2)]))\n",
        "#         extended_keywords_list = extended_keywords_list_oo\n",
        "\n",
        "\n",
        "#     data_dict = {}\n",
        "#     data_dict['new_docs'] = new_data\n",
        "#     data_dict['new_labels'] = new_labels\n",
        "#     data_dict['docs_injected'] = all_injected_docs\n",
        "#     data_dict['labels_injected'] = all_injected_labels\n",
        "#     data_dict['injection_class'] = injection_class\n",
        "#     data_dict['extend_each_by']  = extend_each_by\n",
        "#     data_dict['extended_keywords_list'] = extended_keywords_list\n",
        "#     data_dict['query'] = query\n",
        "#     data_dict['info']  = all_txt_inj\n",
        "\n",
        "#     os.makedirs(home_dir+'/content/data_'+d_data+\"/\"+dtype+\"/injected_docs/\"+str(frac),exist_ok=True)\n",
        "#     # os.makedirs(home_dir+'/content/data_'+d_data+\"/\"+dtype+\"/injected_docs/\"+str(frac),exist_ok=True)\n",
        "\n",
        "#     os.chdir(home_dir+'/content/data_'+d_data+\"/\"+dtype+\"/injected_docs/\"+str(frac))\n",
        "#     compressed_pickle(data_dict,'injected_'+d_data+\"_\"+'_'.join(query)+\"_injClass_\"+'_'.join(injection_class))"
      ],
      "metadata": {
        "id": "7Pr00oIBayjj",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extended_keywords_list_joined = np.asarray([list(extended_keywords_list_oo[x]) + (list(extended_keywords_list_oo[x+1])) for x in range(len(extended_keywords_list_oo)) if x%2==0])\n",
        "# extended_keywords_list_joined = [np.asarray(e) for e in extended_keywords_list_joined]"
      ],
      "metadata": {
        "id": "ReIuYtNWVXQC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### IR dataset"
      ],
      "metadata": {
        "id": "RvVbpfEzQjGp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !rm -r /home/grad16/sakumar/ICDM_Experiments_2022/WTM/SavedOutput/"
      ],
      "metadata": {
        "id": "mgwZe20iEnHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if d_data == 'opinions_twitter' or d_data == 'nfcorpus':\n",
        "  frac = 0.2\n",
        "  data_dict = {}\n",
        "  df['docs_preprocessed_vocab_filtered'] = df['docs_preprocessed'].apply(lambda x: vocab_filtered_data(x,vocab) )\n",
        "  df = df[df['docs_preprocessed_vocab_filtered'].isin(data_preprocessed)].reset_index(drop=True)\n",
        "\n",
        "  df_all_qdocs = df[df['queries']==injection_class].reset_index(drop=True)\n",
        "  opinions_twitter_ground_truth_docs = df_all_qdocs[df_all_qdocs['relevance']=='T']['docs_preprocessed_vocab_filtered'].values\n",
        "\n",
        "  df_all_qdocs = df[df['queries']==injection_class].reset_index(drop=True)\n",
        "  nfcorpus_ground_truth_docs = df_all_qdocs[df_all_qdocs['relevance']==2]['docs_preprocessed_vocab_filtered'].values\n",
        "\n",
        "\n",
        "  # ground_truth_docs = df[df['queries']==injection_class[0]]['docs_preprocessed'].values\n",
        "  nfcorpus_ground_truth_labels = 'MED'+df_all_qdocs[df_all_qdocs['relevance']==2]['relevance'].astype('str').values\n",
        "  opinions_twitter_ground_truth_labels = df_all_qdocs[df_all_qdocs['relevance']=='T']['relevance'].values\n",
        "\n",
        "  extend_each_by,extended_keywords_list = get_extended_keywords_list(data_preprocessed,query,query,embeddings,avg_doc_len)\n",
        "  data_dict['new_docs'] = df['docs_preprocessed_vocab_filtered'].tolist()\n",
        "  data_dict['new_labels'] = df['labels'].tolist()\n",
        "\n",
        "\n",
        "  if d_data == 'opinions_twitter':\n",
        "    data_dict['docs_injected'] = opinions_twitter_ground_truth_docs\n",
        "    data_dict['labels_injected'] = opinions_twitter_ground_truth_labels\n",
        "    data_dict['info']  = str(len(opinions_twitter_ground_truth_docs))+' relevant docs for query '+str(query)\n",
        "\n",
        "  elif d_data == 'nfcorpus':\n",
        "    data_dict['docs_injected'] = nfcorpus_ground_truth_docs\n",
        "    data_dict['labels_injected'] = nfcorpus_ground_truth_labels\n",
        "    data_dict['info']  = str(len(nfcorpus_ground_truth_docs))+' directly relevant docs for query '+str(query)\n",
        "\n",
        "  data_dict['injection_class'] = injection_class\n",
        "  data_dict['extend_each_by']  = extend_each_by\n",
        "  data_dict['extended_keywords_list'] = extended_keywords_list\n",
        "  data_dict['query'] = query\n",
        "\n",
        "  os.makedirs(home_dir+'/content/data_'+d_data+\"/\"+dtype+\"/injected_docs/\"+str(frac),exist_ok=True)\n",
        "\n",
        "  os.chdir(home_dir+'/content/data_'+d_data+\"/\"+dtype+\"/injected_docs/\"+str(frac))\n",
        "  compressed_pickle(data_dict,'injected_'+d_data+\"_\"+'_'.join(query)+\"_injClass_\"+'_'.join(injection_class.split(' ')))"
      ],
      "metadata": {
        "id": "17dWqB_txHZs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from collections import Counter\n",
        "Counter(data_preprocessed_labels)"
      ],
      "metadata": {
        "id": "5JC_Cx7HeSJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# files = os.listdir('.')\n",
        "# d = decompress_pickle(files[0].split('.')[0])"
      ],
      "metadata": {
        "id": "TjWqkj95I0ey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "STOP!!"
      ],
      "metadata": {
        "id": "K22RXBEozGbF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title opinions_twitter docs (relv and irrelv)\n",
        "opinions_twitter_iran_nuclear_relvdocs = np.array([['F',\n",
        "        'iran worked nuclear bomb design iaea iran worked developing nuclear weapon design resear http'],\n",
        "       ['F',\n",
        "        'connecting nuclear dot iran connecting nuclear dot iran regard http'],\n",
        "       ['F',\n",
        "        'demand iran respond day nuclear report reuters demand iran respond day nuclear http'],\n",
        "       ['F',\n",
        "        'barak nuclear iran launch arm race usa nuclear iran launch arm video def'],\n",
        "       ['F',\n",
        "        'power close deal iaea iran resolution http attack iran anti iran scenario sanction nuclear program'],\n",
        "       ['F',\n",
        "        'forget stuxnet israel considering striking iran nuclear program nuclear weapon http'],\n",
        "       ['T', 'israel iran nuclear weapon http state nuclear weapon'],\n",
        "       ['F', 'imo iran develop nuclear weapon develop nuclear weapon'],\n",
        "       ['F',\n",
        "        'truth freed iran nuclear bomb zionist regime israel nuclear warhead threat'],\n",
        "       ['F',\n",
        "        'iaea claim iran developing nuclear bomb http respect resisted pressure claim iraq nuclear'],\n",
        "       ['F',\n",
        "        'time nuclear watchdog admits suspicion iran atomic energy program build nuclear weapon http'],\n",
        "       ['F',\n",
        "        'time nuclear watchdog admits suspicion iran atomic energy program build nuclear weapon http'],\n",
        "       ['T',\n",
        "        'problem iranian nuclear issue faith game nuclear chicken israel iran'],\n",
        "       ['F',\n",
        "        'iran nuclear program keep playing drama pakistan nuclear accepting http'],\n",
        "       ['F',\n",
        "        'iran nuclear defiance tap deep point pride cbs news nuclear defiance tap deep point http'],\n",
        "       ['F',\n",
        "        'iran nuclear site observed sign cleanup http globe nuclear site bei http'],\n",
        "       ['T', 'don terrorist mullah iran nuclear bomb terrorism iran'],\n",
        "       ['F',\n",
        "        'israel strike iran speculation fed looming report iran nuclear activity http'],\n",
        "       ['T',\n",
        "        'iran missile nuclear site iran difficult managing price crude oil gaza'],\n",
        "       ['F', 'report cite secret nuclear iran http digg iran'],\n",
        "       ['F', 'report cite secret nuclear iran http digg iran'],\n",
        "       ['F',\n",
        "        'breakingnews iran president iran won retreat iota nuclear program'],\n",
        "       ['F', 'iran build ethic nuclear bomb http iran'],\n",
        "       ['F',\n",
        "        'mike photographic proof iran nuclear laboratory http iran iaea nuke humor'],\n",
        "       ['T',\n",
        "        'iran nuclear report alarming white house http political iran'],\n",
        "       ['T', 'elect barackobama iran nuclear weapon elect iran'],\n",
        "       ['F',\n",
        "        'domestic election iran news iran nuclear capability agreed stunt'],\n",
        "       ['F', 'iran western nuclear sanction vain iran canada http'],\n",
        "       ['F',\n",
        "        'iran arrest cia spy accused targeting nuclear iran usa cia http'],\n",
        "       ['F', 'word iran nuclear weapon shut strait oil barrel iran'],\n",
        "       ['T',\n",
        "        'harboring illusion iran peaceful intent nuclear power iran'],\n",
        "       ['T', 'iran nuke claim credible iran nuclear program http'],\n",
        "       ['F',\n",
        "        'iran threat closely israel warning window opportunity iran nuclear closing http'],\n",
        "       ['T',\n",
        "        'israel warning iran quiet nod gulf http linked shared fear iran nuclear program'],\n",
        "       ['F',\n",
        "        'concern exist iran mullah easily obtaining nuclear weapon http iran terrorism hezbollah'],\n",
        "       ['T',\n",
        "        'russia issue stark warning attack iran iaea expected iran nearing nuclear capability http'],\n",
        "       ['F',\n",
        "        'iran working advanced warhead http publish detail advanced design iran nuclear warhead'],\n",
        "       ['F',\n",
        "        'iran worked nuclear bomb design watchdog vienna reuters iran appears work http'],\n",
        "       ['F',\n",
        "        'yahoo news iran worked nuclear bomb design news report iran worked http'],\n",
        "       ['T',\n",
        "        'iran nuclear chief american pawn iran president criticized head http'],\n",
        "       ['T',\n",
        "        'strong kay iran president iran won retreat iota nuclear program'],\n",
        "       ['F',\n",
        "        'serious concern iran nuke atomic agency serious concern iran nuclear'],\n",
        "       ['T',\n",
        "        'eng iran serious proof nuclear drive israel support crippling sanction iran http'],\n",
        "       ['T',\n",
        "        'iran hit nuclear weapon allegation president iran retreat http'],\n",
        "       ['F',\n",
        "        'report complicate iran defense nuclear program iran dismissed united nation report http'],\n",
        "       ['F',\n",
        "        'obama iran nuclear program main alternative military operation iran http'],\n",
        "       ['T',\n",
        "        'iran vow counter israeli strike iron fist metaphor survived silly iran iron age nuclear age'],\n",
        "       ['T', 'obama iran failure http obama fail iran nuclear cluster'],\n",
        "       ['F',\n",
        "        'redding news demand iran respond day nuclear report reuters demand iran respond day http'],\n",
        "       ['T',\n",
        "        'elect barack obama iran nuclear weapon elect mitt romney iran http'],\n",
        "       ['T',\n",
        "        'romney war keep iran nuclear weapon http obama iran nuke ows dry'],\n",
        "       ['F',\n",
        "        'jonathan cook striking iran israel war wager http palestine iran nuclear occu'],\n",
        "       ['T',\n",
        "        'iran dismisses iaea report meaningless iran review operation nuclear watchdog http']],\n",
        "      dtype=object)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "dUykLDAhwR5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### save"
      ],
      "metadata": {
        "id": "YPVGy3_mB5ZK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(home_dir)\n",
        "d_data = 'yahooanswers'\n",
        "dtype='short'\n",
        "# data_to_get = 'yahooanswers'\n",
        "# max_features = 'all_vocab'\n",
        "zipped_pickle_filename = 'FoTo_'+d_data+\"_\"+dtype\n",
        "os.system('zip -r '+zipped_pickle_filename+'.zip '+'./content/data_'+d_data)\n",
        "# files.download(zipped_pickle_filename+'_'+str(max_features)+'_.zip')"
      ],
      "metadata": {
        "id": "3Vp9D_9pLMFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## twitter pick queries"
      ],
      "metadata": {
        "id": "9QznJmZdQmRg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d_data = 'opinions_twitter'\n",
        "dtype = 'short'\n",
        "os.chdir(home_dir+'/content/data_'+d_data+\"/\"+dtype)\n",
        "data_preprocessed = load_obj_pkl5('data_preprocessed_'+d_data+'_'+dtype)\n",
        "data_preprocessed_labels = load_obj_pkl5('data_preprocessed_labels_'+d_data+'_'+dtype)\n",
        "df = load_obj_pkl5('dataframe_'+d_data+'_'+dtype)\n",
        "embeddings = load_obj_pkl5('embeddings_'+d_data+'_'+dtype)\n",
        "vocab = load_obj_pkl5('vocab_'+d_data+'_'+dtype)"
      ],
      "metadata": {
        "id": "m14VQ3kFQwcI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings['nuclear']"
      ],
      "metadata": {
        "id": "A4kltgW5tfIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[['iran','nuclear'],\n",
        "['obama'],\n",
        "['manchester'],\n",
        "['pixar']]\n",
        "\n",
        "# ['iran nuclear','Obama,'manchester city','pixar']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3A6bVdgtdHC",
        "outputId": "8e5c1471-f1fb-4e79-a53a-cdebc968a04d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['iran', 'nuclear'], ['obama'], ['manchester'], ['pixar']]"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = df.groupby(['queries','relevance']).size().reset_index(name='counts')\n",
        "df3 = df2[df2['relevance']=='T'].reset_index(drop=True)\n",
        "df3.sort_values(by='counts',ascending=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9yCy3O93Q3qA",
        "outputId": "5043917b-5651-42f5-96ae-9297a8b98481"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                  queries relevance  counts\n",
              "26            job hunting         T      28\n",
              "28                 kinect         T      22\n",
              "2        Jennifer Aniston         T      21\n",
              "1           Breaking Dawn         T      20\n",
              "24           iran nuclear         T      19\n",
              "15             chris paul         T      19\n",
              "7                   Obama         T      19\n",
              "13               big bang         T      15\n",
              "31        manchester city         T      15\n",
              "19                    htc         T      14\n",
              "4            Mac book pro         T      14\n",
              "10             UK embassy         T      13\n",
              "38     speech recognition         T      12\n",
              "37                  pixar         T      11\n",
              "27            kindle fire         T      10\n",
              "14             bill gates         T       9\n",
              "8                   Syria         T       8\n",
              "36            paul graham         T       7\n",
              "0   American Music Awards         T       7\n",
              "20         iOS5 Jailbreak         T       7\n",
              "17            galaxy note         T       7\n",
              "9      Two And A Half Men         T       7\n",
              "29                 lenovo         T       6\n",
              "5                Maggie Q         T       6\n",
              "23                   iran         T       6\n",
              "18         google venture         T       5\n",
              "6       Manchester United         T       5\n",
              "39             steve jobs         T       5\n",
              "25                 itouch         T       5\n",
              "16                 disney         T       4\n",
              "35               owl city         T       4\n",
              "11              UK strike         T       3\n",
              "12               Xbox 360         T       3\n",
              "32              microsoft         T       3\n",
              "33         new Google Bar         T       2\n",
              "34          new start-ups         T       2\n",
              "21             inside job         T       1\n",
              "22               iphone4s         T       1\n",
              "3              Kai-Fu Lee         T       1\n",
              "30       machine learning         T       1"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>queries</th>\n",
              "      <th>relevance</th>\n",
              "      <th>counts</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>job hunting</td>\n",
              "      <td>T</td>\n",
              "      <td>28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>kinect</td>\n",
              "      <td>T</td>\n",
              "      <td>22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Jennifer Aniston</td>\n",
              "      <td>T</td>\n",
              "      <td>21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Breaking Dawn</td>\n",
              "      <td>T</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>iran nuclear</td>\n",
              "      <td>T</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>chris paul</td>\n",
              "      <td>T</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Obama</td>\n",
              "      <td>T</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>big bang</td>\n",
              "      <td>T</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>manchester city</td>\n",
              "      <td>T</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>htc</td>\n",
              "      <td>T</td>\n",
              "      <td>14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Mac book pro</td>\n",
              "      <td>T</td>\n",
              "      <td>14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>UK embassy</td>\n",
              "      <td>T</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>speech recognition</td>\n",
              "      <td>T</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>pixar</td>\n",
              "      <td>T</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>kindle fire</td>\n",
              "      <td>T</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>bill gates</td>\n",
              "      <td>T</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Syria</td>\n",
              "      <td>T</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>paul graham</td>\n",
              "      <td>T</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>American Music Awards</td>\n",
              "      <td>T</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>iOS5 Jailbreak</td>\n",
              "      <td>T</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>galaxy note</td>\n",
              "      <td>T</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Two And A Half Men</td>\n",
              "      <td>T</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>lenovo</td>\n",
              "      <td>T</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Maggie Q</td>\n",
              "      <td>T</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>iran</td>\n",
              "      <td>T</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>google venture</td>\n",
              "      <td>T</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Manchester United</td>\n",
              "      <td>T</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>steve jobs</td>\n",
              "      <td>T</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>itouch</td>\n",
              "      <td>T</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>disney</td>\n",
              "      <td>T</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>owl city</td>\n",
              "      <td>T</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>UK strike</td>\n",
              "      <td>T</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Xbox 360</td>\n",
              "      <td>T</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>microsoft</td>\n",
              "      <td>T</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>new Google Bar</td>\n",
              "      <td>T</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>new start-ups</td>\n",
              "      <td>T</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>inside job</td>\n",
              "      <td>T</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>iphone4s</td>\n",
              "      <td>T</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Kai-Fu Lee</td>\n",
              "      <td>T</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>machine learning</td>\n",
              "      <td>T</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "top_directlinks_queries = df3.sort_values(by='counts',ascending=False)['queries'].values[:5]\n",
        "top_directlinks_queries"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xz_3blGqRCpQ",
        "outputId": "cd066dcb-90aa-49e3-b3b6-967370903e73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['job hunting', 'kinect', 'Jennifer Aniston', 'Breaking Dawn',\n",
              "       'iran nuclear'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = ['job','hunting']\n",
        "injection_class = 'job hunting'\n",
        "\n",
        "query = ['kinect']\n",
        "injection_class = 'kinect'"
      ],
      "metadata": {
        "id": "CJEvKJzwRa_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# queryset = 1\n",
        "# frac = 0.2\n",
        "# os.chdir(home_dir+'/content/data_'+d_data+\"/\"+dtype+'/injected_docs/'+str(frac))\n",
        "# files = os.listdir('.')\n",
        "# data_dict = decompress_pickle(files[queryset-1].split('.')[0])\n",
        "# data_preprocessed = data_dict['new_docs']\n",
        "# data_preprocessed_labels = data_dict['new_labels']\n",
        "# ground_truth_docs = data_dict['docs_injected']\n",
        "# ground_truth_labels = data_dict['labels_injected']\n",
        "# keywords = data_dict['query']\n",
        "# extended_keywords_list = data_dict['extended_keywords_list']"
      ],
      "metadata": {
        "id": "F38nsyOPQwcJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## nfcorpus pick queries"
      ],
      "metadata": {
        "id": "NY7YQxgOhgdL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(home_dir)"
      ],
      "metadata": {
        "id": "eptu1BuAQjRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d_data = 'nfcorpus'\n",
        "dtype = 'short'\n",
        "os.chdir(home_dir+'/content/data_'+d_data+\"/\"+dtype)\n",
        "data_preprocessed = load_obj_pkl5('data_preprocessed_'+d_data+'_'+dtype)\n",
        "data_preprocessed_labels = load_obj_pkl5('data_preprocessed_labels_'+d_data+'_'+dtype)\n",
        "df = load_obj_pkl5('dataframe_'+d_data+'_'+dtype)\n",
        "embeddings = load_obj_pkl5('embeddings_'+d_data+'_'+dtype)\n",
        "vocab = load_obj_pkl5('vocab_'+d_data+'_'+dtype)"
      ],
      "metadata": {
        "id": "jKaSKy8VQjRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = df.groupby(['queries','relevance']).size().reset_index(name='counts')"
      ],
      "metadata": {
        "id": "cIpQ7_xmQjRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df3 = df2[df2['relevance']==2].reset_index(drop=True)\n",
        "df3.sort_values(by='counts',ascending=False)['queries'].values"
      ],
      "metadata": {
        "id": "G8bmpeouQjRr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df3.sort_values(by='counts',ascending=False).head(20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 677
        },
        "id": "zknAzvh7CWSw",
        "outputId": "02305ca3-08d4-4e1f-d1ef-58f15f5198bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               queries  relevance  counts\n",
              "111  what does the research say about dietary inter...          2      20\n",
              "27                                coffee and mortality          2      17\n",
              "96   treating dry eye disease with diet : just add ...          2      15\n",
              "53   how many bowel movements should you have every...          2      15\n",
              "59               how to boost the benefits of exercise          2      13\n",
              "109                             what causes diabetes ?          2      11\n",
              "79            living longer by reducing leucine intake          2      11\n",
              "38             does rye bread protect against cancer ?          2      11\n",
              "85        putrefying protein and “ toxifying ” enzymes          2      10\n",
              "41                        fish consumption and suicide          2      10\n",
              "16     brown fat : losing weight through thermogenesis          2       9\n",
              "92   should you sit , squat , or lean during a bowe...          2       8\n",
              "116              why deep fried foods may cause cancer          2       8\n",
              "44                      foods for macular degeneration          2       8\n",
              "42    flame retardant pollutants and child development          2       8\n",
              "54           how may plants protect against diabetes ?          2       8\n",
              "24                can we fight the blues with greens ?          2       7\n",
              "40              fatty meals may impair artery function          2       7\n",
              "36                  does chocolate cause weight gain ?          2       7\n",
              "82                    organic milk and prostate cancer          2       6"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>queries</th>\n",
              "      <th>relevance</th>\n",
              "      <th>counts</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>111</th>\n",
              "      <td>what does the research say about dietary inter...</td>\n",
              "      <td>2</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>coffee and mortality</td>\n",
              "      <td>2</td>\n",
              "      <td>17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>treating dry eye disease with diet : just add ...</td>\n",
              "      <td>2</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>how many bowel movements should you have every...</td>\n",
              "      <td>2</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59</th>\n",
              "      <td>how to boost the benefits of exercise</td>\n",
              "      <td>2</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>109</th>\n",
              "      <td>what causes diabetes ?</td>\n",
              "      <td>2</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79</th>\n",
              "      <td>living longer by reducing leucine intake</td>\n",
              "      <td>2</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>does rye bread protect against cancer ?</td>\n",
              "      <td>2</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85</th>\n",
              "      <td>putrefying protein and “ toxifying ” enzymes</td>\n",
              "      <td>2</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>fish consumption and suicide</td>\n",
              "      <td>2</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>brown fat : losing weight through thermogenesis</td>\n",
              "      <td>2</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>92</th>\n",
              "      <td>should you sit , squat , or lean during a bowe...</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>116</th>\n",
              "      <td>why deep fried foods may cause cancer</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>foods for macular degeneration</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>flame retardant pollutants and child development</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54</th>\n",
              "      <td>how may plants protect against diabetes ?</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>can we fight the blues with greens ?</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>fatty meals may impair artery function</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>does chocolate cause weight gain ?</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82</th>\n",
              "      <td>organic milk and prostate cancer</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "top_directlinks_queries = df3.sort_values(by='counts',ascending=False)['queries'].values[:20]\n",
        "top_directlinks_queries"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xvU2WsVQjRr",
        "outputId": "e1aaf16d-8b37-4ce7-93fd-0322e4312d3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['what does the research say about dietary interventions on women with pcos ?',\n",
              "       'coffee and mortality',\n",
              "       'treating dry eye disease with diet : just add water ?',\n",
              "       'how many bowel movements should you have every day ?',\n",
              "       'how to boost the benefits of exercise', 'what causes diabetes ?',\n",
              "       'living longer by reducing leucine intake',\n",
              "       'does rye bread protect against cancer ?',\n",
              "       'putrefying protein and “ toxifying ” enzymes',\n",
              "       'fish consumption and suicide',\n",
              "       'brown fat : losing weight through thermogenesis',\n",
              "       'should you sit , squat , or lean during a bowel movement ?',\n",
              "       'why deep fried foods may cause cancer',\n",
              "       'foods for macular degeneration',\n",
              "       'flame retardant pollutants and child development',\n",
              "       'how may plants protect against diabetes ?',\n",
              "       'can we fight the blues with greens ?',\n",
              "       'fatty meals may impair artery function',\n",
              "       'does chocolate cause weight gain ?',\n",
              "       'organic milk and prostate cancer'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query_words = top_directlinks_queries[0].split(' ')\n",
        "query_words_w_emb = []\n",
        "for q in query_words:\n",
        "  try:\n",
        "    embeddings[q]\n",
        "    query_words_w_emb.append(q)\n",
        "  except:\n",
        "    pass"
      ],
      "metadata": {
        "id": "GabSpLF0QjRr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_words #  dietary intervention woman"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9744679-0e2f-44d1-a8cb-aa8a37e3f3cf",
        "id": "xgF77rZMQjRr"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['what',\n",
              " 'does',\n",
              " 'the',\n",
              " 'research',\n",
              " 'say',\n",
              " 'about',\n",
              " 'dietary',\n",
              " 'interventions',\n",
              " 'on',\n",
              " 'women',\n",
              " 'with',\n",
              " 'pcos',\n",
              " '?']"
            ]
          },
          "metadata": {},
          "execution_count": 151
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# df[df['queries']=='what does the research say about dietary interventions on women with pcos ?']\n",
        "# df[df['queries']=='coffee and mortality']"
      ],
      "metadata": {
        "id": "xy6wX9WhQjRr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = ['dietary', 'intervention', 'woman']\n",
        "injection_class = 'what does the research say about dietary interventions on women with pcos ?'\n",
        "\n",
        "query = ['coffee','mortality']\n",
        "injection_class = 'coffee and mortality'"
      ],
      "metadata": {
        "id": "hzfqeHpZIuLF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### old queries\n"
      ],
      "metadata": {
        "id": "OA5PCK4bSN_J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "### nfcorpus\n",
        "# query = ['benefit','exercise']\n",
        "# injection_class = ['how to boost the benefits of exercise']\n",
        "\n",
        "# query = ['food','macular','degeneration']\n",
        "# injection_class = ['foods for macular degeneration']\n",
        "\n",
        "### opinions_twitter\n",
        "# query = ['iran','nuclear']\n",
        "# injection_class = ['iran nuclear']\n",
        "\n",
        "# query = ['big','bang']\n",
        "# injection_class = ['big bang']"
      ],
      "metadata": {
        "id": "C8PsLXXsSPla"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}