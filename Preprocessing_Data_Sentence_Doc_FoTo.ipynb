{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOzTCkbmwvgcQhOdZLc/z7K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanujsriv/_topic_models/blob/main/Preprocessing_Data_Sentence_Doc_FoTo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkoX5Yna0xYk"
      },
      "source": [
        "#Data preprocessing (No need to run)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yD-MvQv20wkx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcf6f3a1-fb70-4709-8b70-52502602defd"
      },
      "source": [
        "## @title Download Stopwords , punkt, wordnet\n",
        "import nltk\n",
        "# nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "import torch\n",
        "import pandas as pd\n",
        "from torch import nn\n",
        "import re\n",
        "from time import time\n",
        "import numpy as np\n",
        "import collections\n",
        "import torch.optim as optim\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "import sklearn\n",
        "import re\n",
        "import string\n",
        "from numpy import random\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import word_tokenize\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "stem = PorterStemmer()\n",
        "wnl = WordNetLemmatizer()\n",
        "stopwords = ['cant','better','well','going','will','would','know','dont','get','like','think','im',\"also\",\"said\",\"a\", \"able\", \"about\", \"above\", \"abst\", \"accordance\", \"according\", \"accordingly\", \"across\", \"act\", \"actually\", \"added\", \"adj\", \"affected\", \"affecting\", \"affects\", \"after\", \"afterwards\", \"again\", \"against\", \"ah\", \"all\", \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\", \"am\", \"among\", \"amongst\", \"an\", \"and\", \"announce\", \"another\", \"any\", \"anybody\", \"anyhow\", \"anymore\", \"anyone\", \"anything\", \"anyway\", \"anyways\", \"anywhere\", \"apparently\", \"approximately\", \"are\", \"aren\", \"arent\", \"arise\", \"around\", \"as\", \"aside\", \"ask\", \"asking\", \"at\", \"auth\", \"available\", \"away\", \"awfully\", \"b\", \"back\", \"be\", \"became\", \"because\", \"become\", \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"begin\", \"beginning\", \"beginnings\", \"begins\", \"behind\", \"being\", \"believe\", \"below\", \"beside\", \"besides\", \"between\", \"beyond\", \"biol\", \"both\", \"brief\", \"briefly\", \"but\", \"by\", \"c\", \"ca\", \"came\", \"can\", \"cannot\", \"can't\", \"cause\", \"causes\", \"certain\", \"certainly\", \"co\", \"com\", \"come\", \"comes\", \"contain\", \"containing\", \"contains\", \"could\", \"couldnt\", \"d\", \"date\", \"did\", \"didn't\", \"different\", \"do\", \"does\", \"doesn't\", \"doing\", \"done\", \"don't\", \"down\", \"downwards\", \"due\", \"during\", \"e\", \"each\", \"ed\", \"edu\", \"effect\", \"eg\", \"eight\", \"eighty\", \"either\", \"else\", \"elsewhere\", \"end\", \"ending\", \"enough\", \"especially\", \"et\", \"et-al\", \"etc\", \"even\", \"ever\", \"every\", \"everybody\", \"everyone\", \"everything\", \"everywhere\", \"ex\", \"except\", \"f\", \"far\", \"few\", \"ff\", \"fifth\", \"first\", \"five\", \"fix\", \"followed\", \"following\", \"follows\", \"for\", \"former\", \"formerly\", \"forth\", \"found\", \"four\", \"from\", \"further\", \"furthermore\", \"g\", \"gave\", \"get\", \"gets\", \"getting\", \"give\", \"given\", \"gives\", \"giving\", \"go\", \"goes\", \"gone\", \"got\", \"gotten\", \"h\", \"had\", \"happens\", \"hardly\", \"has\", \"hasn't\", \"have\", \"haven't\", \"having\", \"he\", \"hed\", \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"heres\", \"hereupon\", \"hers\", \"herself\", \"hes\", \"hi\", \"hid\", \"him\", \"himself\", \"his\", \"hither\", \"home\", \"how\", \"howbeit\", \"however\", \"hundred\", \"i\", \"id\", \"ie\", \"if\", \"i'll\", \"im\", \"immediate\", \"immediately\", \"importance\", \"important\", \"in\", \"inc\", \"indeed\", \"index\", \"information\", \"instead\", \"into\", \"invention\", \"inward\", \"is\", \"isn't\", \"it\", \"itd\", \"it'll\", \"its\", \"itself\", \"i've\", \"j\", \"just\", \"k\", \"keep  keeps\", \"kept\", \"kg\", \"km\", \"know\", \"known\", \"knows\", \"l\", \"largely\", \"last\", \"lately\", \"later\", \"latter\", \"latterly\", \"least\", \"less\", \"lest\", \"let\", \"lets\", \"like\", \"liked\", \"likely\", \"line\", \"little\", \"'ll\", \"look\", \"looking\", \"looks\", \"ltd\", \"m\", \"made\", \"mainly\", \"make\", \"makes\", \"many\", \"may\", \"maybe\", \"me\", \"mean\", \"means\", \"meantime\", \"meanwhile\", \"merely\", \"mg\", \"might\", \"million\", \"miss\", \"ml\", \"more\", \"moreover\", \"most\", \"mostly\", \"mr\", \"mrs\", \"much\", \"mug\", \"must\", \"my\", \"myself\", \"n\", \"na\", \"name\", \"namely\", \"nay\", \"nd\", \"near\", \"nearly\", \"necessarily\", \"necessary\", \"need\", \"needs\", \"neither\", \"never\", \"nevertheless\", \"new\", \"next\", \"nine\", \"ninety\", \"no\", \"nobody\", \"non\", \"none\", \"nonetheless\", \"noone\", \"nor\", \"normally\", \"nos\", \"not\", \"noted\", \"nothing\", \"now\", \"nowhere\", \"o\", \"obtain\", \"obtained\", \"obviously\", \"of\", \"off\", \"often\", \"oh\", \"ok\", \"okay\", \"old\", \"omitted\", \"on\", \"once\", \"one\", \"ones\", \"only\", \"onto\", \"or\", \"ord\", \"other\", \"others\", \"otherwise\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"outside\", \"over\", \"overall\", \"owing\", \"own\", \"p\", \"page\", \"pages\", \"part\", \"particular\", \"particularly\", \"past\", \"per\", \"perhaps\", \"placed\", \"please\", \"plus\", \"poorly\", \"possible\", \"possibly\", \"potentially\", \"pp\", \"predominantly\", \"present\", \"previously\", \"primarily\", \"probably\", \"promptly\", \"proud\", \"provides\", \"put\", \"q\", \"que\", \"quickly\", \"quite\", \"qv\", \"r\", \"ran\", \"rather\", \"rd\", \"re\", \"readily\", \"really\", \"recent\", \"recently\", \"ref\", \"refs\", \"regarding\", \"regardless\", \"regards\", \"related\", \"relatively\", \"research\", \"respectively\", \"resulted\", \"resulting\", \"results\", \"right\", \"run\", \"s\", \"said\", \"same\", \"saw\", \"say\", \"saying\", \"says\", \"sec\", \"section\", \"see\", \"seeing\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"seen\", \"self\", \"selves\", \"sent\", \"seven\", \"several\", \"shall\", \"she\", \"shed\", \"she'll\", \"shes\", \"should\", \"shouldn't\", \"show\", \"showed\", \"shown\", \"showns\", \"shows\", \"significant\", \"significantly\", \"similar\", \"similarly\", \"since\", \"six\", \"slightly\", \"so\", \"some\", \"somebody\", \"somehow\", \"someone\", \"somethan\", \"something\", \"sometime\", \"sometimes\", \"somewhat\", \"somewhere\", \"soon\", \"sorry\", \"specifically\", \"specified\", \"specify\", \"specifying\", \"still\", \"stop\", \"strongly\", \"sub\", \"substantially\", \"successfully\", \"such\", \"sufficiently\", \"suggest\", \"sup\", \"sure    t\", \"take\", \"taken\", \"taking\", \"tell\", \"tends\", \"th\", \"than\", \"thank\", \"thanks\", \"thanx\", \"that\", \"that'll\", \"thats\", \"that've\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\", \"thered\", \"therefore\", \"therein\", \"there'll\", \"thereof\", \"therere\", \"theres\", \"thereto\", \"thereupon\", \"there've\", \"these\", \"they\", \"theyd\", \"they'll\", \"theyre\", \"they've\", \"think\", \"this\", \"those\", \"thou\", \"though\", \"thoughh\", \"thousand\", \"throug\", \"through\", \"throughout\", \"thru\", \"thus\", \"til\", \"tip\", \"to\", \"together\", \"too\", \"took\", \"toward\", \"towards\", \"tried\", \"tries\", \"truly\", \"try\", \"trying\", \"ts\", \"twice\", \"two\", \"u\", \"un\", \"under\", \"unfortunately\", \"unless\", \"unlike\", \"unlikely\", \"until\", \"unto\", \"up\", \"upon\", \"ups\", \"us\", \"use\", \"used\", \"useful\", \"usefully\", \"usefulness\", \"uses\", \"using\", \"usually\", \"v\", \"value\", \"various\", \"'ve\", \"very\", \"via\", \"viz\", \"vol\", \"vols\", \"vs\", \"w\", \"want\", \"wants\", \"was\", \"wasnt\", \"way\", \"we\", \"wed\", \"welcome\", \"we'll\", \"went\", \"were\", \"werent\", \"we've\", \"what\", \"whatever\", \"what'll\", \"whats\", \"when\", \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"wheres\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whim\", \"whither\", \"who\", \"whod\", \"whoever\", \"whole\", \"who'll\", \"whom\", \"whomever\", \"whos\", \"whose\", \"why\", \"widely\", \"willing\", \"wish\", \"with\", \"within\", \"without\", \"wont\", \"words\", \"world\", \"would\", \"wouldnt\", \"www\", \"x\", \"y\", \"yes\", \"yet\", \"you\", \"youd\", \"you'll\", \"your\", \"youre\", \"yours\", \"yourself\", \"yourselves\", \"you've\", \"z\", \"zero\"]\n",
        "\n",
        "def preprocessing_nonstem(doc,word2vec_model,my_punctuation):\n",
        "    # print(doc)\n",
        "    # word_vectors = word2vec_model.wv\n",
        "    doc = doc.lower()\n",
        "    # print(doc)\n",
        "    doc = doc.translate(str.maketrans(string.punctuation, ' '*len(string.punctuation)))\n",
        "    doc = word_tokenize(doc)\n",
        "    doc = filter(lambda x: x not in my_punctuation, doc)\n",
        "    # doc = filter(lambda x:x not in stopwords, doc)\n",
        "    doc = filter(lambda x:not x.isdigit(), doc)\n",
        "    doc = [wnl.lemmatize(w.lower()) for w in doc]\n",
        "    doc = [stem.stem(w) for w in doc]\n",
        "    # doc = filter(lambda x: x in word2vec_model.vocab,doc)\n",
        "    doc = filter(lambda x:x not in stopwords, doc)\n",
        "    doc = filter(lambda x: x in word2vec_model.vocab or x in \".\",doc)\n",
        "    # doc = filter(lambda x:x not in stopwords, doc)\n",
        "    doc = ' '.join(e for e in doc)\n",
        "    return doc"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJZtEpPf1Ubk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66c8a542-715a-4524-f062-7559e7969ee1"
      },
      "source": [
        "!wget -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"\n",
        "!gunzip GoogleNews-vectors-negative300.bin.gz\n",
        "from gensim import models\n",
        "word2vec_model = models.KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin', binary=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-05-20 04:33:08--  https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.92.237\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.92.237|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1647046227 (1.5G) [application/x-gzip]\n",
            "Saving to: ‘GoogleNews-vectors-negative300.bin.gz’\n",
            "\n",
            "GoogleNews-vectors- 100%[===================>]   1.53G  84.7MB/s    in 22s     \n",
            "\n",
            "2021-05-20 04:33:30 (71.9 MB/s) - ‘GoogleNews-vectors-negative300.bin.gz’ saved [1647046227/1647046227]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "usMtzpC06t1s"
      },
      "source": [
        "#@title function : load / save pickle_obj\n",
        "import pickle\n",
        "\n",
        "def save_obj(obj, name):\n",
        "    with open(name + '.pkl', 'wb') as f:\n",
        "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "def load_obj(name):\n",
        "    with open(name + '.pkl', 'rb') as f:\n",
        "        return pickle.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zE0B7qPFGha7"
      },
      "source": [
        "# !wget http://jwebpro.sourceforge.net/data-web-snippets.tar.gz\n",
        "# !tar -xvzf data-web-snippets.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DiS4RiCnHy7O"
      },
      "source": [
        "# corpus = []\n",
        "# with open('/content/data-web-snippets/train.txt', \"rb\") as f:\n",
        "#   content = f.readlines()\n",
        "#   content = [x.lower().decode('ISO-8859-1') for x in content]\n",
        "#   corpus.append(''.join(content))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNSb18zcHH_h"
      },
      "source": [
        "# import os\n",
        "# import glob\n",
        "# def get_bbc_data():\n",
        "#   os.system('wget http://jwebpro.sourceforge.net/data-web-snippets.tar.gz')\n",
        "#   os.system('tar -xvzf data-web-snippets.tar.gz')\n",
        "\n",
        "#   # SearchSnippets Docs -\n",
        "#   corpus = []\n",
        "\n",
        "#   # labels\n",
        "#   return corpus,labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVnRxJaCslhe"
      },
      "source": [
        "import os\n",
        "import glob\n",
        "def get_bbc_data():\n",
        "  # os.system('wget https://www.dropbox.com/s/vunli21d312x55g/bbc.zip')\n",
        "  # os.system('wget https://www.dropbox.com/s/h6y9zfdb76gl4uz/bbc-fulltext.zip')\n",
        "  # os.system('unzip bbc.zip')\n",
        "  # os.system('unzip bbc-fulltext.zip')\n",
        "\n",
        "  # BBC Docs -\n",
        "  corpus = []\n",
        "  subfolders = [f.path for f in os.scandir(os.getcwd()+'/bbc') if f.is_dir()]\n",
        "  subfolders = sorted(subfolders)\n",
        "  for s in subfolders:\n",
        "    files_list = sorted(glob.glob(s+\"/*.txt\"))\n",
        "    for file in files_list:\n",
        "      with open(file, \"rb\") as f:\n",
        "        content = f.readlines()\n",
        "        content = [content[0],content[2]] # headlines and abstracts\n",
        "        content = [x.strip().lower().decode('ISO-8859-1') for x in content]\n",
        "        corpus.append(' '.join(content).strip())\n",
        "\n",
        "  # BBC_labels -\n",
        "  with open(\"bbc.classes\", \"r\") as f:\n",
        "    content = f.readlines()\n",
        "    content = [x.strip()[::-1][0] for x in content]\n",
        "    labels = content[4:]\n",
        "    label_dict = {'0':'business','1':'entertainment','2':'politics','3':'sport','4':'tech'}\n",
        "  for l in range(len(labels)):\n",
        "    labels[l] = label_dict[labels[l]]\n",
        "\n",
        "  return corpus,labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxidHJvujq2D"
      },
      "source": [
        "docs,labels = get_bbc_data()\n",
        "len(docs),len(labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zVczW6QVrFfF",
        "outputId": "640bf207-7495-4b87-80ef-cfe03566f168"
      },
      "source": [],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('business', 510),\n",
              " ('entertainment', 386),\n",
              " ('politics', 417),\n",
              " ('sport', 511),\n",
              " ('tech', 401)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lH2pSst53XyM"
      },
      "source": [
        "dots = [(\". \"*k).strip() for k in range(1,100)]\n",
        "def docs_labels_preprocessing(docs,labels,word2vec_model,my_punctuation):\n",
        "  data_preprocessed = []\n",
        "  data_preprocessed_labels = []\n",
        "  for i in range(len(docs)):\n",
        "    doc = preprocessing_nonstem(docs[i],word2vec_model,my_punctuation)\n",
        "    # doc[i] = docs[i].replace(\".\",\"\")\n",
        "    if(doc!='' and (doc not in dots)):\n",
        "      data_preprocessed.append(doc)\n",
        "      data_preprocessed_labels.append(labels[i])\n",
        "  return data_preprocessed,data_preprocessed_labels\n",
        "\n",
        "def docs_sentences_labels_preprocessing(docs,labels,word2vec_model):\n",
        "  doc_preprocessed = []\n",
        "  doc_preprocessed_labels = []\n",
        "  sen_preprocessed = []\n",
        "  doc_id_sent = []\n",
        "  for i in range(len(docs)):\n",
        "    if(i%1000==0): print(i)\n",
        "    doc = ''\n",
        "    sen_list = nltk.tokenize.sent_tokenize(docs[i])\n",
        "    for j in sen_list:\n",
        "      sen = preprocessing_nonstem(j,word2vec_model,string.punctuation)\n",
        "      if(sen!='' and (sen not in dots)):\n",
        "        sen_preprocessed.append(sen)\n",
        "        doc_id_sent.append(len(doc_preprocessed))\n",
        "        doc = (doc + ' ' + sen).strip()\n",
        "    if(doc!='' and (doc not in dots)):\n",
        "      doc_preprocessed.append(doc)\n",
        "      doc_preprocessed_labels.append(labels[i])\n",
        "  return doc_preprocessed,doc_preprocessed_labels,sen_preprocessed,doc_id_sent"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KvUOqln1wk1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2544c1e2-03bb-490a-c8e8-9179252a1ae0"
      },
      "source": [
        "doc_preprocessed,doc_preprocessed_labels,sen_preprocessed,doc_id_sent=docs_sentences_labels_preprocessing(docs,labels,word2vec_model)\n",
        "# save_obj(doc_preprocessed,'doc_preprocessed')\n",
        "# save_obj(doc_preprocessed_labels, 'doc_preprocessed_labels')\n",
        "# save_obj(sen_preprocessed,'sen_preprocessed')\n",
        "# save_obj(doc_id_sent,'doc_id_sent')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "1000\n",
            "2000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XthlCZTzRnuZ"
      },
      "source": [
        "def create_doc_sentences(doc_id_sent):\n",
        "  doc_sentences = []\n",
        "  num_doc = max(doc_id_sent)+1\n",
        "  for i in range(num_doc):\n",
        "    if(i%1000==0):\n",
        "      print(i)\n",
        "    indices = [j for j, x in enumerate(doc_id_sent) if x == i]\n",
        "    doc_sentences.append(indices)\n",
        "  return doc_sentences\n",
        "\n",
        "def preprocessed_data(sens, docs, labels, word2vec_model):\n",
        "  vectorizer = CountVectorizer(dtype=np.float32)\n",
        "  train_vec = vectorizer.fit_transform(sens).toarray()\n",
        "  vocab = vectorizer.vocabulary_\n",
        "  nonzeros_indexes = np.where(train_vec.any(1))[0]\n",
        "  train_vec_non_zeros = [train_vec[i] for i in nonzeros_indexes]\n",
        "  sens_non_zeros = [sens[i] for i in nonzeros_indexes]\n",
        "  doc_sent_id_non_zeroes = [doc_id_sent[i] for i in nonzeros_indexes]\n",
        "\n",
        "  train_label =[]\n",
        "  keep_docs = []\n",
        "  doc_preprocessed_nonzeroes = []\n",
        "\n",
        "  for i in doc_sent_id_non_zeroes:\n",
        "    if(i not in keep_docs):\n",
        "      keep_docs.append(i)\n",
        "  for i,j in enumerate(keep_docs):\n",
        "    if(i%1000==0): print(i)\n",
        "    doc_preprocessed_nonzeroes.append(doc_preprocessed[j])\n",
        "    train_label.append(labels[j])\n",
        "\n",
        "    for k,l in enumerate(doc_sent_id_non_zeroes):\n",
        "      if(l==j): doc_sent_id_non_zeroes[k]=i\n",
        "\n",
        "  embeddings = {}\n",
        "  for f in vocab:\n",
        "    embeddings[f] = word2vec_model[f]\n",
        "\n",
        "  save_obj(embeddings,'embeddings')\n",
        "  save_obj(doc_sent_id_non_zeroes, 'doc_id_sent_nonzeros')\n",
        "  save_obj(doc_preprocessed_nonzeroes,'doc_preprocessed_nonzeroes')\n",
        "  save_obj(train_label,'doc_preprocessed_nonzeroes_labels')\n",
        "  save_obj(sens_non_zeros, 'sen_preprocessed_nonzeroes')\n",
        "  save_obj(train_vec, 'train_vec')\n",
        "  save_obj(train_vec_non_zeros, 'train_vec_non_zeros')\n",
        "  save_obj(vocab, 'vocab')\n",
        "  return sens_non_zeros, train_label, train_vec_non_zeros, vocab, doc_sent_id_non_zeroes, embeddings"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAsWgB4sUXjA"
      },
      "source": [
        "rm -r *.pkl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAxopfrcmgrl"
      },
      "source": [
        "import gc\n",
        "sens_non_zeros, train_label, train_vec_non_zeros, vocab, doc_sent_id_non_zeroes, embeddings = preprocessed_data(sen_preprocessed,doc_preprocessed,doc_preprocessed_labels,word2vec_model)\n",
        "doc_sentences = create_doc_sentences(doc_sent_id_non_zeroes)\n",
        "save_obj(doc_sentences,'doc_sentences')\n",
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jvkxejFy4oe"
      },
      "source": [
        "!zip data_bbc_headlines_abstracts.zip *.pkl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUN-kXACfgBK"
      },
      "source": [
        "# !zip train_vec.zip train_vec.pkl\n",
        "# !zip train_vec_non_zeros.zip train_vec_non_zeros.pkl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBRe8eK20R7t",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "42e9d67b-7ed9-41e6-b23a-e298cc6adfba"
      },
      "source": [
        "from google.colab import files\n",
        "files.download(\"data_bbc_headlines_abstract.zip\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_e9159cd4-8d99-48ab-8db0-20221f434797\", \"data_bbc_headlines_abstract.zip\", 3906182)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hC7YhIYiAWkb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "outputId": "e9b109a3-07ff-44b5-b1d7-430cb1c4f70b"
      },
      "source": [
        "Score Not Used Anymore\n",
        "Score Not Used Anymore\n",
        "Score Not Used Anymore"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-19-7debe0a30bea>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    Score Not Used Anymore\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQAYr-SEYEsN"
      },
      "source": [
        "def cosine_similarity_desm_docs(query_list, train_vec, vocab, embeddings):\n",
        "  num_docs = len(train_vec)\n",
        "  num_voc = len(vocab)\n",
        "  sim_list = torch.zeros(num_docs)\n",
        "  index = 0\n",
        "  id_vocab = dict(map(reversed, vocab.items()))\n",
        "  for d in range(num_docs):\n",
        "    if(d%5000==0): print(d)\n",
        "    doc_bar = torch.zeros(300)\n",
        "    doc_length = 0\n",
        "    for v in range(num_voc):\n",
        "      if(train_vec[d][v]>0):\n",
        "        doc_bar.add_(train_vec[d][v] * torch.from_numpy(embeddings[id_vocab[v]])/torch.norm(torch.from_numpy(embeddings[id_vocab[v]])))\n",
        "        doc_length = doc_length + train_vec[d][v]\n",
        "    doc_bar = doc_bar / doc_length\n",
        "    sum = 0\n",
        "\n",
        "    for q in query_list:\n",
        "      sum += torch.dot(torch.from_numpy(embeddings[q]) , doc_bar)/(torch.norm(torch.from_numpy(embeddings[q]))*torch.norm(doc_bar))\n",
        "    sum = sum/len(query_list)\n",
        "    sim_list[index]=sum\n",
        "    index = index + 1\n",
        "\n",
        "  return sim_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxdjcTr9X_J6"
      },
      "source": [
        "keywords = get_keywords()\n",
        "all_rscores = cosine_similarity_desm_docs(keywords, train_vec_non_zeros, vocab, embeddings)\n",
        "save_obj(all_rscores, 'all_rscores')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}